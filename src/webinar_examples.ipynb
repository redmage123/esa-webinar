{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "57e83402-3d13-4426-970b-89f2b99c6e49",
   "metadata": {},
   "source": [
    "## Model 1: Convolutional Neural Network (CNN) with Attention Layer\n",
    "\n",
    "This section demonstrates how to build and train a Convolutional Neural Network (CNN) with an attention mechanism. The model is designed for image classification tasks. We will use the Keras library to construct the model, compile it, and train it on a dataset.\n",
    "\n",
    "### Steps:\n",
    "1. **Build the CNN model with an attention layer.**\n",
    "2. **Compile the model with an appropriate optimizer and loss function.**\n",
    "3. **Train the model on a given dataset.**\n",
    "4. **Plot the training accuracy and loss.**\n",
    "5. **Evaluate the model's performance.**\n",
    "6. **Visualize the attention weights on sample images.**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7f3f3257-0733-4344-9742-ea1be09d93d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from typing import Tuple, List\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "139189a2-27ba-40b8-b2de-53c749deba3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.16.1\n",
      "Num GPUs Available:  0\n",
      "False\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "print(tf.__version__)\n",
    "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))\n",
    "print(tf.test.is_built_with_cuda())\n",
    "print(tf.config.list_physical_devices('GPU'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c1e2a97f-cae6-4a9d-8e02-37ec7952f43b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow version: 2.16.1\n",
      "Built with CUDA: False\n",
      "GPUs visible to TensorFlow: []\n",
      "GPU available: False\n",
      "Successfully performed a GPU operation\n",
      "CUDA version not found in environment variables\n",
      "cuDNN version not available (TensorFlow not built with CUDA)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import os\n",
    "\n",
    "def check_tensorflow_gpu():\n",
    "    print(f\"TensorFlow version: {tf.__version__}\")\n",
    "\n",
    "    # Check if TensorFlow is built with CUDA\n",
    "    print(f\"Built with CUDA: {tf.test.is_built_with_cuda()}\")\n",
    "\n",
    "    # Check if TensorFlow can see any GPUs\n",
    "    gpus = tf.config.list_physical_devices('GPU')\n",
    "    print(f\"GPUs visible to TensorFlow: {gpus}\")\n",
    "\n",
    "    # Check if GPU is available (this is different from just being visible)\n",
    "    print(f\"GPU available: {tf.test.is_gpu_available()}\")\n",
    "\n",
    "    # If GPUs are available, print some information about them\n",
    "    if gpus:\n",
    "        for gpu in gpus:\n",
    "            print(f\"GPU name: {gpu.name}\")\n",
    "            print(f\"GPU device type: {gpu.device_type}\")\n",
    "\n",
    "    # Try to perform a simple operation on GPU\n",
    "    try:\n",
    "        with tf.device('/GPU:0'):\n",
    "            a = tf.constant([[1.0, 2.0, 3.0], [4.0, 5.0, 6.0]])\n",
    "            b = tf.constant([[1.0, 2.0], [3.0, 4.0], [5.0, 6.0]])\n",
    "            c = tf.matmul(a, b)\n",
    "        print(\"Successfully performed a GPU operation\")\n",
    "    except RuntimeError as e:\n",
    "        print(f\"Failed to perform GPU operation: {e}\")\n",
    "\n",
    "    # Print CUDA version if available\n",
    "    if 'CUDA_VERSION' in os.environ:\n",
    "        print(f\"CUDA version: {os.environ['CUDA_VERSION']}\")\n",
    "    else:\n",
    "        print(\"CUDA version not found in environment variables\")\n",
    "\n",
    "    # Print cuDNN version if available\n",
    "    if tf.test.is_built_with_cuda():\n",
    "        print(f\"cuDNN version: {tf.test.gpu_device_name()}\")\n",
    "    else:\n",
    "        print(\"cuDNN version not available (TensorFlow not built with CUDA)\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    check_tensorflow_gpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0d42cb45-22f1-44aa-a46f-46b671d94baa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'torch' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 24\u001b[0m\n\u001b[0;32m     21\u001b[0m class_names \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mairplane\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mautomobile\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbird\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcat\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdeer\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdog\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfrog\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhorse\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mship\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtruck\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m     23\u001b[0m \u001b[38;5;66;03m# If you need to access the raw data (similar to the original cell)\u001b[39;00m\n\u001b[1;32m---> 24\u001b[0m train_data \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241m.\u001b[39mtensor(trainset\u001b[38;5;241m.\u001b[39mdata)\u001b[38;5;241m.\u001b[39mfloat() \u001b[38;5;241m/\u001b[39m \u001b[38;5;241m255.0\u001b[39m\n\u001b[0;32m     25\u001b[0m test_data \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(testset\u001b[38;5;241m.\u001b[39mdata)\u001b[38;5;241m.\u001b[39mfloat() \u001b[38;5;241m/\u001b[39m \u001b[38;5;241m255.0\u001b[39m\n\u001b[0;32m     26\u001b[0m train_labels \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(trainset\u001b[38;5;241m.\u001b[39mtargets)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'torch' is not defined"
     ]
    }
   ],
   "source": [
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Define transforms\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "])\n",
    "\n",
    "# Load CIFAR-10 dataset\n",
    "trainset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
    "testset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)\n",
    "\n",
    "# Create data loaders\n",
    "batch_size = 64  # You can adjust this\n",
    "trainloader = DataLoader(trainset, batch_size=batch_size, shuffle=True, num_workers=2)\n",
    "testloader = DataLoader(testset, batch_size=batch_size, shuffle=False, num_workers=2)\n",
    "\n",
    "# Class names for CIFAR-10 dataset (to simulate satellite image classes)\n",
    "class_names = ['airplane', 'automobile', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']\n",
    "\n",
    "# If you need to access the raw data (similar to the original cell)\n",
    "train_data = torch.tensor(trainset.data).float() / 255.0\n",
    "test_data = torch.tensor(testset.data).float() / 255.0\n",
    "train_labels = torch.tensor(trainset.targets)\n",
    "test_labels = torch.tensor(testset.targets)\n",
    "\n",
    "# Move to GPU if available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "train_data = train_data.to(device)\n",
    "test_data = test_data.to(device)\n",
    "train_labels = train_labels.to(device)\n",
    "test_labels = test_labels.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1b5869b2-7172-4a19-b57e-4b47a490ac68",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_cnn_model() -> models.Sequential:\n",
    "    \"\"\"\n",
    "    Creates a Convolutional Neural Network (CNN) model.\n",
    "\n",
    "    Returns:\n",
    "    models.Sequential: The constructed CNN model.\n",
    "    \"\"\"\n",
    "    model = models.Sequential()\n",
    "    # Define the input shape using Input layer\n",
    "    model.add(layers.Input(shape=(32, 32, 3)))\n",
    "    # Add convolutional layer with 32 filters, 3x3 kernel size, and ReLU activation\n",
    "    model.add(layers.Conv2D(32, (3, 3), activation='relu'))\n",
    "    # Add max pooling layer with 2x2 pool size\n",
    "    model.add(layers.MaxPooling2D((2, 2)))\n",
    "    # Add convolutional layer with 64 filters, 3x3 kernel size, and ReLU activation\n",
    "    model.add(layers.Conv2D(64, (3, 3), activation='relu'))\n",
    "    # Add max pooling layer with 2x2 pool size\n",
    "    model.add(layers.MaxPooling2D((2, 2)))\n",
    "    # Add another convolutional layer with 64 filters, 3x3 kernel size, and ReLU activation\n",
    "    model.add(layers.Conv2D(64, (3, 3), activation='relu'))\n",
    "    \n",
    "    # Flatten the output of the convolutional layers to feed into fully connected layers\n",
    "    model.add(layers.Flatten())\n",
    "    # Add fully connected (Dense) layer with 64 units and ReLU activation\n",
    "    model.add(layers.Dense(64, activation='relu'))\n",
    "    # Add output layer with 10 units (for 10 classes) and softmax activation for probability output\n",
    "    model.add(layers.Dense(10, activation='softmax'))\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "94b7d102-91c4-4cd6-b408-412c59942729",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "1563/1563 [==============================] - 27s 16ms/step - loss: 1.5221 - accuracy: 0.4462 - val_loss: 1.2325 - val_accuracy: 0.5522\n",
      "Epoch 2/100\n",
      "1563/1563 [==============================] - 24s 15ms/step - loss: 1.1482 - accuracy: 0.5929 - val_loss: 1.0762 - val_accuracy: 0.6184\n",
      "Epoch 3/100\n",
      "1563/1563 [==============================] - 27s 18ms/step - loss: 0.9944 - accuracy: 0.6503 - val_loss: 1.0092 - val_accuracy: 0.6458\n",
      "Epoch 4/100\n",
      "1563/1563 [==============================] - 31s 20ms/step - loss: 0.8935 - accuracy: 0.6893 - val_loss: 0.9418 - val_accuracy: 0.6654\n",
      "Epoch 5/100\n",
      "1563/1563 [==============================] - 36s 23ms/step - loss: 0.8204 - accuracy: 0.7117 - val_loss: 0.8802 - val_accuracy: 0.6920\n",
      "Epoch 6/100\n",
      "1563/1563 [==============================] - 38s 24ms/step - loss: 0.7663 - accuracy: 0.7323 - val_loss: 0.8811 - val_accuracy: 0.6971\n",
      "Epoch 7/100\n",
      "1563/1563 [==============================] - 34s 22ms/step - loss: 0.7174 - accuracy: 0.7485 - val_loss: 0.8817 - val_accuracy: 0.6958\n",
      "Epoch 8/100\n",
      "1563/1563 [==============================] - 29s 18ms/step - loss: 0.6701 - accuracy: 0.7648 - val_loss: 0.8615 - val_accuracy: 0.7064\n",
      "Epoch 9/100\n",
      "1563/1563 [==============================] - 28s 18ms/step - loss: 0.6329 - accuracy: 0.7776 - val_loss: 0.8238 - val_accuracy: 0.7211\n",
      "Epoch 10/100\n",
      "1563/1563 [==============================] - 29s 18ms/step - loss: 0.5979 - accuracy: 0.7895 - val_loss: 0.8661 - val_accuracy: 0.7104\n",
      "Epoch 11/100\n",
      "1563/1563 [==============================] - 29s 19ms/step - loss: 0.5609 - accuracy: 0.8021 - val_loss: 0.8823 - val_accuracy: 0.7119\n",
      "Epoch 12/100\n",
      "1563/1563 [==============================] - 30s 19ms/step - loss: 0.5356 - accuracy: 0.8104 - val_loss: 0.9012 - val_accuracy: 0.7161\n",
      "Epoch 13/100\n",
      "1563/1563 [==============================] - 27s 17ms/step - loss: 0.5054 - accuracy: 0.8219 - val_loss: 0.9080 - val_accuracy: 0.7117\n",
      "Epoch 14/100\n",
      "1563/1563 [==============================] - 28s 18ms/step - loss: 0.4788 - accuracy: 0.8276 - val_loss: 0.9947 - val_accuracy: 0.7036\n",
      "Epoch 15/100\n",
      "1563/1563 [==============================] - 27s 18ms/step - loss: 0.4528 - accuracy: 0.8399 - val_loss: 0.9535 - val_accuracy: 0.7134\n",
      "Epoch 16/100\n",
      "1563/1563 [==============================] - 28s 18ms/step - loss: 0.4255 - accuracy: 0.8478 - val_loss: 1.0045 - val_accuracy: 0.7044\n",
      "Epoch 17/100\n",
      "1563/1563 [==============================] - 27s 18ms/step - loss: 0.4022 - accuracy: 0.8567 - val_loss: 0.9884 - val_accuracy: 0.7213\n",
      "Epoch 18/100\n",
      "1563/1563 [==============================] - 28s 18ms/step - loss: 0.3831 - accuracy: 0.8632 - val_loss: 1.1102 - val_accuracy: 0.6935\n",
      "Epoch 19/100\n",
      "1563/1563 [==============================] - 27s 18ms/step - loss: 0.3661 - accuracy: 0.8679 - val_loss: 1.1066 - val_accuracy: 0.7059\n",
      "Epoch 20/100\n",
      "1563/1563 [==============================] - 28s 18ms/step - loss: 0.3411 - accuracy: 0.8776 - val_loss: 1.1681 - val_accuracy: 0.6986\n",
      "Epoch 21/100\n",
      "1563/1563 [==============================] - 29s 18ms/step - loss: 0.3220 - accuracy: 0.8834 - val_loss: 1.1846 - val_accuracy: 0.7006\n",
      "Epoch 22/100\n",
      "1563/1563 [==============================] - 28s 18ms/step - loss: 0.3052 - accuracy: 0.8909 - val_loss: 1.2167 - val_accuracy: 0.7009\n",
      "Epoch 23/100\n",
      "1563/1563 [==============================] - 28s 18ms/step - loss: 0.2866 - accuracy: 0.8967 - val_loss: 1.3156 - val_accuracy: 0.7005\n",
      "Epoch 24/100\n",
      "1563/1563 [==============================] - 28s 18ms/step - loss: 0.2732 - accuracy: 0.9027 - val_loss: 1.3304 - val_accuracy: 0.6949\n",
      "Epoch 25/100\n",
      "1563/1563 [==============================] - 28s 18ms/step - loss: 0.2584 - accuracy: 0.9077 - val_loss: 1.3989 - val_accuracy: 0.6981\n",
      "Epoch 26/100\n",
      "1563/1563 [==============================] - 28s 18ms/step - loss: 0.2487 - accuracy: 0.9107 - val_loss: 1.4242 - val_accuracy: 0.6927\n",
      "Epoch 27/100\n",
      "1563/1563 [==============================] - 31s 20ms/step - loss: 0.2309 - accuracy: 0.9166 - val_loss: 1.5080 - val_accuracy: 0.6981\n",
      "Epoch 28/100\n",
      "1563/1563 [==============================] - 26s 17ms/step - loss: 0.2219 - accuracy: 0.9203 - val_loss: 1.5837 - val_accuracy: 0.6957\n",
      "Epoch 29/100\n",
      "1563/1563 [==============================] - 26s 17ms/step - loss: 0.2175 - accuracy: 0.9218 - val_loss: 1.5479 - val_accuracy: 0.6997\n",
      "Epoch 30/100\n",
      "1563/1563 [==============================] - 27s 17ms/step - loss: 0.2022 - accuracy: 0.9288 - val_loss: 1.6775 - val_accuracy: 0.6923\n",
      "Epoch 31/100\n",
      "1563/1563 [==============================] - 27s 18ms/step - loss: 0.1991 - accuracy: 0.9283 - val_loss: 1.6571 - val_accuracy: 0.6939\n",
      "Epoch 32/100\n",
      "1563/1563 [==============================] - 27s 17ms/step - loss: 0.1921 - accuracy: 0.9316 - val_loss: 1.7775 - val_accuracy: 0.6849\n",
      "Epoch 33/100\n",
      "1563/1563 [==============================] - 26s 17ms/step - loss: 0.1831 - accuracy: 0.9332 - val_loss: 1.8729 - val_accuracy: 0.6976\n",
      "Epoch 34/100\n",
      "1563/1563 [==============================] - 24s 16ms/step - loss: 0.1756 - accuracy: 0.9358 - val_loss: 1.8621 - val_accuracy: 0.6959\n",
      "Epoch 35/100\n",
      "1563/1563 [==============================] - 24s 16ms/step - loss: 0.1739 - accuracy: 0.9373 - val_loss: 1.9167 - val_accuracy: 0.6847\n",
      "Epoch 36/100\n",
      "1563/1563 [==============================] - 26s 17ms/step - loss: 0.1654 - accuracy: 0.9403 - val_loss: 1.9180 - val_accuracy: 0.7011\n",
      "Epoch 37/100\n",
      "1563/1563 [==============================] - 24s 16ms/step - loss: 0.1658 - accuracy: 0.9404 - val_loss: 2.0805 - val_accuracy: 0.6794\n",
      "Epoch 38/100\n",
      "1563/1563 [==============================] - 24s 16ms/step - loss: 0.1520 - accuracy: 0.9460 - val_loss: 2.1141 - val_accuracy: 0.6846\n",
      "Epoch 39/100\n",
      "1563/1563 [==============================] - 25s 16ms/step - loss: 0.1529 - accuracy: 0.9455 - val_loss: 2.1541 - val_accuracy: 0.6878\n",
      "Epoch 40/100\n",
      "1563/1563 [==============================] - 26s 16ms/step - loss: 0.1536 - accuracy: 0.9455 - val_loss: 2.0876 - val_accuracy: 0.6877\n",
      "Epoch 41/100\n",
      "1563/1563 [==============================] - 26s 17ms/step - loss: 0.1502 - accuracy: 0.9473 - val_loss: 2.1815 - val_accuracy: 0.6833\n",
      "Epoch 42/100\n",
      "1563/1563 [==============================] - 25s 16ms/step - loss: 0.1371 - accuracy: 0.9501 - val_loss: 2.2352 - val_accuracy: 0.6884\n",
      "Epoch 43/100\n",
      "1563/1563 [==============================] - 24s 15ms/step - loss: 0.1415 - accuracy: 0.9510 - val_loss: 2.2119 - val_accuracy: 0.6844\n",
      "Epoch 44/100\n",
      "1563/1563 [==============================] - 24s 15ms/step - loss: 0.1356 - accuracy: 0.9524 - val_loss: 2.3286 - val_accuracy: 0.6867\n",
      "Epoch 45/100\n",
      "1563/1563 [==============================] - 25s 16ms/step - loss: 0.1383 - accuracy: 0.9515 - val_loss: 2.3966 - val_accuracy: 0.6898\n",
      "Epoch 46/100\n",
      "1563/1563 [==============================] - 26s 17ms/step - loss: 0.1298 - accuracy: 0.9545 - val_loss: 2.3925 - val_accuracy: 0.6885\n",
      "Epoch 47/100\n",
      "1563/1563 [==============================] - 24s 16ms/step - loss: 0.1272 - accuracy: 0.9545 - val_loss: 2.3619 - val_accuracy: 0.6839\n",
      "Epoch 48/100\n",
      "1563/1563 [==============================] - 25s 16ms/step - loss: 0.1295 - accuracy: 0.9548 - val_loss: 2.5946 - val_accuracy: 0.6809\n",
      "Epoch 49/100\n",
      "1563/1563 [==============================] - 25s 16ms/step - loss: 0.1303 - accuracy: 0.9552 - val_loss: 2.4141 - val_accuracy: 0.6915\n",
      "Epoch 50/100\n",
      "1563/1563 [==============================] - 24s 16ms/step - loss: 0.1216 - accuracy: 0.9570 - val_loss: 2.6101 - val_accuracy: 0.6763\n",
      "Epoch 51/100\n",
      "1563/1563 [==============================] - 24s 16ms/step - loss: 0.1216 - accuracy: 0.9573 - val_loss: 2.6072 - val_accuracy: 0.6851\n",
      "Epoch 52/100\n",
      "1563/1563 [==============================] - 24s 16ms/step - loss: 0.1284 - accuracy: 0.9558 - val_loss: 2.6075 - val_accuracy: 0.6889\n",
      "Epoch 53/100\n",
      "1563/1563 [==============================] - 26s 17ms/step - loss: 0.1159 - accuracy: 0.9596 - val_loss: 2.6375 - val_accuracy: 0.6890\n",
      "Epoch 54/100\n",
      "1563/1563 [==============================] - 25s 16ms/step - loss: 0.1133 - accuracy: 0.9611 - val_loss: 2.7288 - val_accuracy: 0.6840\n",
      "Epoch 55/100\n",
      "1563/1563 [==============================] - 26s 17ms/step - loss: 0.1181 - accuracy: 0.9602 - val_loss: 2.7479 - val_accuracy: 0.6856\n",
      "Epoch 56/100\n",
      "1563/1563 [==============================] - 26s 16ms/step - loss: 0.1183 - accuracy: 0.9588 - val_loss: 2.7031 - val_accuracy: 0.6849\n",
      "Epoch 57/100\n",
      "1563/1563 [==============================] - 25s 16ms/step - loss: 0.1082 - accuracy: 0.9624 - val_loss: 2.8238 - val_accuracy: 0.6795\n",
      "Epoch 58/100\n",
      "1563/1563 [==============================] - 25s 16ms/step - loss: 0.1154 - accuracy: 0.9602 - val_loss: 2.9217 - val_accuracy: 0.6720\n",
      "Epoch 59/100\n",
      "1563/1563 [==============================] - 24s 16ms/step - loss: 0.1035 - accuracy: 0.9643 - val_loss: 2.8458 - val_accuracy: 0.6825\n",
      "Epoch 60/100\n",
      "1563/1563 [==============================] - 26s 17ms/step - loss: 0.1119 - accuracy: 0.9631 - val_loss: 2.8608 - val_accuracy: 0.6868\n",
      "Epoch 61/100\n",
      "1563/1563 [==============================] - 30s 19ms/step - loss: 0.1165 - accuracy: 0.9610 - val_loss: 2.8965 - val_accuracy: 0.6852\n",
      "Epoch 62/100\n",
      "1563/1563 [==============================] - 37s 24ms/step - loss: 0.1098 - accuracy: 0.9622 - val_loss: 2.8424 - val_accuracy: 0.6816\n",
      "Epoch 63/100\n",
      "1563/1563 [==============================] - 36s 23ms/step - loss: 0.0979 - accuracy: 0.9667 - val_loss: 2.8084 - val_accuracy: 0.6848\n",
      "Epoch 64/100\n",
      "1563/1563 [==============================] - 29s 18ms/step - loss: 0.1090 - accuracy: 0.9631 - val_loss: 2.9323 - val_accuracy: 0.6806\n",
      "Epoch 65/100\n",
      "1563/1563 [==============================] - 30s 19ms/step - loss: 0.1050 - accuracy: 0.9649 - val_loss: 2.8853 - val_accuracy: 0.6809\n",
      "Epoch 66/100\n",
      "1563/1563 [==============================] - 24s 15ms/step - loss: 0.1042 - accuracy: 0.9646 - val_loss: 2.9282 - val_accuracy: 0.6845\n",
      "Epoch 67/100\n",
      "1563/1563 [==============================] - 26s 17ms/step - loss: 0.1023 - accuracy: 0.9657 - val_loss: 3.0969 - val_accuracy: 0.6837\n",
      "Epoch 68/100\n",
      "1563/1563 [==============================] - 26s 17ms/step - loss: 0.1087 - accuracy: 0.9647 - val_loss: 3.0715 - val_accuracy: 0.6760\n",
      "Epoch 69/100\n",
      "1563/1563 [==============================] - 26s 16ms/step - loss: 0.0927 - accuracy: 0.9693 - val_loss: 2.9800 - val_accuracy: 0.6819\n",
      "Epoch 70/100\n",
      "1563/1563 [==============================] - 26s 16ms/step - loss: 0.1013 - accuracy: 0.9659 - val_loss: 2.9748 - val_accuracy: 0.6811\n",
      "Epoch 71/100\n",
      "1563/1563 [==============================] - 25s 16ms/step - loss: 0.0954 - accuracy: 0.9679 - val_loss: 3.1055 - val_accuracy: 0.6799\n",
      "Epoch 72/100\n",
      "1563/1563 [==============================] - 29s 18ms/step - loss: 0.0956 - accuracy: 0.9679 - val_loss: 3.2294 - val_accuracy: 0.6833\n",
      "Epoch 73/100\n",
      "1563/1563 [==============================] - 28s 18ms/step - loss: 0.0960 - accuracy: 0.9677 - val_loss: 3.2085 - val_accuracy: 0.6860\n",
      "Epoch 74/100\n",
      "1563/1563 [==============================] - 25s 16ms/step - loss: 0.0968 - accuracy: 0.9675 - val_loss: 3.2263 - val_accuracy: 0.6773\n",
      "Epoch 75/100\n",
      "1563/1563 [==============================] - 31s 20ms/step - loss: 0.0950 - accuracy: 0.9689 - val_loss: 3.2229 - val_accuracy: 0.6753\n",
      "Epoch 76/100\n",
      "1563/1563 [==============================] - 27s 17ms/step - loss: 0.0954 - accuracy: 0.9689 - val_loss: 3.2747 - val_accuracy: 0.6789\n",
      "Epoch 77/100\n",
      "1563/1563 [==============================] - 31s 20ms/step - loss: 0.0956 - accuracy: 0.9688 - val_loss: 3.2461 - val_accuracy: 0.6833\n",
      "Epoch 78/100\n",
      "1563/1563 [==============================] - 30s 19ms/step - loss: 0.0973 - accuracy: 0.9675 - val_loss: 3.1399 - val_accuracy: 0.6820\n",
      "Epoch 79/100\n",
      "1563/1563 [==============================] - 25s 16ms/step - loss: 0.0954 - accuracy: 0.9693 - val_loss: 3.1502 - val_accuracy: 0.6784\n",
      "Epoch 80/100\n",
      "1563/1563 [==============================] - 25s 16ms/step - loss: 0.0904 - accuracy: 0.9705 - val_loss: 3.2172 - val_accuracy: 0.6797\n",
      "Epoch 81/100\n",
      "1563/1563 [==============================] - 25s 16ms/step - loss: 0.0940 - accuracy: 0.9681 - val_loss: 3.3275 - val_accuracy: 0.6828\n",
      "Epoch 82/100\n",
      "1563/1563 [==============================] - 25s 16ms/step - loss: 0.0938 - accuracy: 0.9692 - val_loss: 3.3189 - val_accuracy: 0.6823\n",
      "Epoch 83/100\n",
      "1563/1563 [==============================] - 24s 16ms/step - loss: 0.0873 - accuracy: 0.9714 - val_loss: 3.1833 - val_accuracy: 0.6863\n",
      "Epoch 84/100\n",
      "1563/1563 [==============================] - 25s 16ms/step - loss: 0.0930 - accuracy: 0.9707 - val_loss: 3.2079 - val_accuracy: 0.6832\n",
      "Epoch 85/100\n",
      "1563/1563 [==============================] - 25s 16ms/step - loss: 0.0865 - accuracy: 0.9719 - val_loss: 3.3814 - val_accuracy: 0.6879\n",
      "Epoch 86/100\n",
      "1563/1563 [==============================] - 25s 16ms/step - loss: 0.0918 - accuracy: 0.9702 - val_loss: 3.4734 - val_accuracy: 0.6816\n",
      "Epoch 87/100\n",
      "1563/1563 [==============================] - 25s 16ms/step - loss: 0.0937 - accuracy: 0.9703 - val_loss: 3.2876 - val_accuracy: 0.6777\n",
      "Epoch 88/100\n",
      "1563/1563 [==============================] - 24s 16ms/step - loss: 0.0754 - accuracy: 0.9749 - val_loss: 3.5348 - val_accuracy: 0.6824\n",
      "Epoch 89/100\n",
      "1563/1563 [==============================] - 25s 16ms/step - loss: 0.0972 - accuracy: 0.9693 - val_loss: 3.4573 - val_accuracy: 0.6824\n",
      "Epoch 90/100\n",
      "1563/1563 [==============================] - 25s 16ms/step - loss: 0.0832 - accuracy: 0.9729 - val_loss: 3.6235 - val_accuracy: 0.6824\n",
      "Epoch 91/100\n",
      "1563/1563 [==============================] - 25s 16ms/step - loss: 0.0941 - accuracy: 0.9698 - val_loss: 3.5273 - val_accuracy: 0.6823\n",
      "Epoch 92/100\n",
      "1563/1563 [==============================] - 24s 16ms/step - loss: 0.0898 - accuracy: 0.9717 - val_loss: 3.4547 - val_accuracy: 0.6801\n",
      "Epoch 93/100\n",
      "1563/1563 [==============================] - 25s 16ms/step - loss: 0.0864 - accuracy: 0.9716 - val_loss: 3.4927 - val_accuracy: 0.6774\n",
      "Epoch 94/100\n",
      "1563/1563 [==============================] - 25s 16ms/step - loss: 0.0850 - accuracy: 0.9728 - val_loss: 3.5045 - val_accuracy: 0.6888\n",
      "Epoch 95/100\n",
      "1563/1563 [==============================] - 25s 16ms/step - loss: 0.0851 - accuracy: 0.9733 - val_loss: 3.5259 - val_accuracy: 0.6764\n",
      "Epoch 96/100\n",
      "1563/1563 [==============================] - 28s 18ms/step - loss: 0.0862 - accuracy: 0.9718 - val_loss: 3.4415 - val_accuracy: 0.6857\n",
      "Epoch 97/100\n",
      "1563/1563 [==============================] - 26s 17ms/step - loss: 0.0830 - accuracy: 0.9736 - val_loss: 3.6508 - val_accuracy: 0.6846\n",
      "Epoch 98/100\n",
      "1563/1563 [==============================] - 28s 18ms/step - loss: 0.0842 - accuracy: 0.9738 - val_loss: 3.5053 - val_accuracy: 0.6835\n",
      "Epoch 99/100\n",
      "1563/1563 [==============================] - 27s 17ms/step - loss: 0.0825 - accuracy: 0.9742 - val_loss: 3.5885 - val_accuracy: 0.6844\n",
      "Epoch 100/100\n",
      "1563/1563 [==============================] - 28s 18ms/step - loss: 0.0778 - accuracy: 0.9748 - val_loss: 3.7076 - val_accuracy: 0.6805\n",
      "313/313 - 1s - loss: 3.7076 - accuracy: 0.6805 - 1s/epoch - 5ms/step\n",
      "Test accuracy: 68.05%\n"
     ]
    }
   ],
   "source": [
    "# Assuming train_images, train_labels, test_images, test_labels are predefined datasets\n",
    "\n",
    "# Create the model\n",
    "cnn_model: models.Sequential = create_cnn_model()\n",
    "\n",
    "# Compile the model\n",
    "cnn_model.compile(\n",
    "    optimizer='adam',\n",
    "    loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=False),\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "history: tf.keras.callbacks.History = cnn_model.fit(\n",
    "    train_images,\n",
    "    train_labels,\n",
    "    epochs=100,\n",
    "    validation_data=(test_images, test_labels)\n",
    ")\n",
    "\n",
    "# Evaluate the model\n",
    "def evaluate_model(model: models.Sequential, test_images: np.ndarray, test_labels: np.ndarray) -> Tuple[float, float]:\n",
    "    \"\"\"\n",
    "    Evaluates the CNN model on the test dataset.\n",
    "\n",
    "    Args:\n",
    "    model (models.Sequential): The trained CNN model.\n",
    "    test_images (np.ndarray): Test images.\n",
    "    test_labels (np.ndarray): Test labels.\n",
    "\n",
    "    Returns:\n",
    "    Tuple[float, float]: Test loss and test accuracy.\n",
    "    \"\"\"\n",
    "    test_loss, test_acc = model.evaluate(test_images, test_labels, verbose=2)\n",
    "    return test_loss, test_acc\n",
    "\n",
    "test_loss, test_acc = evaluate_model(cnn_model, test_images, test_labels)\n",
    "print(f'Test accuracy: {test_acc * 100:.2f}%')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dc1f582d-de56-4e99-a002-4b151285e754",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.16.1\n",
      "Num GPUs Available:  0\n",
      "False\n",
      "WARNING:tensorflow:From C:\\Users\\bbrel\\AppData\\Local\\Temp\\ipykernel_10844\\2799446784.py:4: is_gpu_available (from tensorflow.python.framework.test_util) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.config.list_physical_devices('GPU')` instead.\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "print(tf.__version__)\n",
    "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))\n",
    "print(tf.test.is_built_with_cuda())\n",
    "print(tf.test.is_gpu_available(cuda_only=False, min_cuda_compute_capability=None))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf45504e-60b7-4d82-b629-2114af539a52",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "# Create the model\n",
    "cnn_model = create_cnn_model()\n",
    "\n",
    "# Compile the model\n",
    "cnn_model.compile(optimizer='adam',\n",
    "                  loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=False),\n",
    "                  metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "history = cnn_model.fit(train_images, train_labels, epochs=100, \n",
    "                        validation_data=(test_images, test_labels))\n",
    "\n",
    "# Evaluate the model\n",
    "test_loss, test_acc = cnn_model.evaluate(test_images,  test_labels, verbose=2)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90b845c4-9013-4842-a188-68489bfa8b2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model\n",
    "cnn_model.save('../models/cnn_satellite_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f5379421-d1fc-4efc-be86-daa28bf26dc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to rescale images for display\n",
    "def rescale_image(image: np.ndarray) -> np.ndarray:\n",
    "\n",
    "    \"\"\" \n",
    "    Args:\n",
    "    image (np.ndarray): Input image.\n",
    "\n",
    "    Returns:\n",
    "    np.ndarray: Rescaled image.\n",
    "    \"\"\"\n",
    "    image = image / 255.0\n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "dac474f2-6862-4a3c-8cee-ed0a8e5360bf",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'predictions' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[13], line 17\u001b[0m\n\u001b[0;32m     15\u001b[0m plt\u001b[38;5;241m.\u001b[39msubplot(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m2\u001b[39m)\n\u001b[0;32m     16\u001b[0m plt\u001b[38;5;241m.\u001b[39mimshow(rescale_image(test_images[i]))\n\u001b[1;32m---> 17\u001b[0m plt\u001b[38;5;241m.\u001b[39mtitle(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mPredicted: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mclass_names[np\u001b[38;5;241m.\u001b[39margmax(predictions[i])]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     18\u001b[0m plt\u001b[38;5;241m.\u001b[39mshow()\n",
      "\u001b[1;31mNameError\u001b[0m: name 'predictions' is not defined"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x400 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def display_predictions(predictions: np.ndarray, test_images: np.ndarray, test_labels: np.ndarray, class_names: List[str]) -> None:\n",
    "\n",
    "\n",
    "    \"\"\" \n",
    "    Args:\n",
    "    predictions (np.ndarray): Model predictions.\n",
    "    test_images (np.ndarray): Test images.\n",
    "    test_labels (np.ndarray): Actual labels for the test images.\n",
    "    class_names (List[str]): List of class names.\n",
    "    \"\"\"\n",
    "    for i in range(5):\n",
    "        plt.figure(figsize=(10, 4))  \n",
    "        # Display the original image with actual label\n",
    "        plt.subplot(1, 2, 1)\n",
    "        plt.imshow(rescale_image(test_images[i]))\n",
    "        plt.title(f'Actual: {class_names[test_labels[i][0]]}')\n",
    "        # Display the original image with predicted label\n",
    "        plt.subplot(1, 2, 2)\n",
    "        plt.imshow(rescale_image(test_images[i]))\n",
    "        plt.title(f'Predicted: {class_names[np.argmax(predictions[i])]}')\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e39552d-ff42-41ae-bc8a-18299cf30feb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Example predictions\n",
    "predictions: np.ndarray = cnn_model.predict(test_images[:5])\n",
    "\n",
    "# Display predictions with rescaled images\n",
    "def display_predictions(predictions: np.ndarray, test_images: np.ndarray, test_labels: np.ndarray, class_names: List[str]) -> None:\n",
    "    \"\"\"\n",
    "    Displays the actual and predicted labels for a subset of test images.\n",
    "\n",
    "    Args:\n",
    "    predictions (np.ndarray): Model predictions.\n",
    "    test_images (np.ndarray): Test images.\n",
    "    test_labels (np.ndarray): Actual labels for the test images.\n",
    "    class_names (List[str]): List of class names.\n",
    "    \"\"\"\n",
    "    for i in range(5):\n",
    "        plt.figure(figsize=(10, 4))\n",
    "        # Display the original image with actual label\n",
    "        plt.subplot(1, 2, 1)\n",
    "        plt.imshow(rescale_image(test_images[i]))\n",
    "        plt.title(f'Actual: {class_names[test_labels[i][0]]}')\n",
    "        # Display the original image with predicted label\n",
    "        plt.subplot(1, 2, 2)\n",
    "        plt.imshow(rescale_image(test_images[i]))\n",
    "        plt.title(f'Predicted: {class_names[np.argmax(predictions[i])]}')\n",
    "        plt.show()\n",
    "\n",
    "# Assume class_names is predefined list of class names\n",
    "display_predictions(predictions, test_images, test_labels, class_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9623f10a-83ca-475c-9b88-524bf2ab102e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60898c67-8eb8-472b-8d37-ddc24c96089d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Function to rescale images for display\n",
    "def rescale_image(image: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Rescales the image pixel values to the range [0, 1] for display.\n",
    "\n",
    "    Args:\n",
    "    image (np.ndarray): Input image.\n",
    "\n",
    "    Returns:\n",
    "    np.ndarray: Rescaled image.\n",
    "    \"\"\"\n",
    "    image = image / 255.0\n",
    "    return image\n",
    "\n",
    "# Example predictions\n",
    "predictions: np.ndarray = cnn_model.predict(test_images[:5])\n",
    "\n",
    "# Display predictions with rescaled images\n",
    "def display_predictions(predictions: np.ndarray, test_images: np.ndarray, test_labels: np.ndarray, class_names: List[str]) -> None:\n",
    "    \"\"\"\n",
    "    Displays the actual and predicted labels for a subset of test images.\n",
    "\n",
    "    Args:\n",
    "    predictions (np.ndarray): Model predictions.\n",
    "    test_images (np.ndarray): Test images.\n",
    "    test_labels (np.ndarray): Actual labels for the test images.\n",
    "    class_names (List[str]): List of class names.\n",
    "    \"\"\"\n",
    "    for i in range(5):\n",
    "        plt.figure(figsize=(10, 4))\n",
    "        # Display the original image with actual label\n",
    "        plt.subplot(1, 2, 1)\n",
    "        plt.imshow(rescale_image(test_images[i]))\n",
    "        plt.title(f'Actual: {class_names[test_labels[i][0]]}')\n",
    "        # Display the original image with predicted label\n",
    "        plt.subplot(1, 2, 2)\n",
    "        plt.imshow(rescale_image(test_images[i]))\n",
    "        plt.title(f'Predicted: {class_names[np.argmax(predictions[i])]}')\n",
    "        plt.show()\n",
    "\n",
    "# Assume class_names is predefined list of class names\n",
    "display_predictions(predictions, test_images, test_labels, class_names)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a33300e3-b8a8-4cc5-8e29-0d148d5ce1eb",
   "metadata": {},
   "source": [
    "## Generative Adversarial Network (GAN)\n",
    "\n",
    "This section demonstrates how to create, compile, train, and generate images using a Generative Adversarial Network (GAN). GANs consist of two neural networks, a generator and a discriminator, that compete against each other to produce realistic images.\n",
    "\n",
    "### Steps:\n",
    "1. **Create the generator and discriminator models.**\n",
    "2. **Compile the models with appropriate optimizers and loss functions.**\n",
    "3. **Train the GAN on a dataset of images.**\n",
    "4. **Generate and display new images using the trained generator.**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a7cf5317-435b-4db3-a0f9-1e216c497ddf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total images loaded: 27000\n",
      "0 [D loss: 0.8468660116195679 | G loss: 1.6887357234954834]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[25], line 131\u001b[0m\n\u001b[0;32m    128\u001b[0m batch_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m64\u001b[39m\n\u001b[0;32m    130\u001b[0m \u001b[38;5;66;03m# Train the GAN\u001b[39;00m\n\u001b[1;32m--> 131\u001b[0m \u001b[43mtrain_gan\u001b[49m\u001b[43m(\u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1000\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample_interval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[25], line 105\u001b[0m, in \u001b[0;36mtrain_gan\u001b[1;34m(epochs, batch_size, sample_interval)\u001b[0m\n\u001b[0;32m    103\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(X_train) \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m batch_size):\n\u001b[0;32m    104\u001b[0m     real_images \u001b[38;5;241m=\u001b[39m X_train[i \u001b[38;5;241m*\u001b[39m batch_size:(i \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m*\u001b[39m batch_size]\n\u001b[1;32m--> 105\u001b[0m     d_loss, g_loss \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreal_images\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    107\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m epoch \u001b[38;5;241m%\u001b[39m sample_interval \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m    108\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m [D loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00md_loss\u001b[38;5;241m.\u001b[39mnumpy()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m | G loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mg_loss\u001b[38;5;241m.\u001b[39mnumpy()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m]\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\users\\bbrel\\esa_webinar\\.venv\\lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    148\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    149\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 150\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32mc:\\users\\bbrel\\esa_webinar\\.venv\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\polymorphic_function.py:833\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    830\u001b[0m compiler \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxla\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnonXla\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    832\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m OptionalXlaContext(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile):\n\u001b[1;32m--> 833\u001b[0m   result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[0;32m    835\u001b[0m new_tracing_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexperimental_get_tracing_count()\n\u001b[0;32m    836\u001b[0m without_tracing \u001b[38;5;241m=\u001b[39m (tracing_count \u001b[38;5;241m==\u001b[39m new_tracing_count)\n",
      "File \u001b[1;32mc:\\users\\bbrel\\esa_webinar\\.venv\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\polymorphic_function.py:869\u001b[0m, in \u001b[0;36mFunction._call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    866\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n\u001b[0;32m    867\u001b[0m   \u001b[38;5;66;03m# In this case we have created variables on the first call, so we run the\u001b[39;00m\n\u001b[0;32m    868\u001b[0m   \u001b[38;5;66;03m# defunned version which is guaranteed to never create variables.\u001b[39;00m\n\u001b[1;32m--> 869\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtracing_compilation\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    870\u001b[0m \u001b[43m      \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_no_variable_creation_config\u001b[49m\n\u001b[0;32m    871\u001b[0m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    872\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_variable_creation_config \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    873\u001b[0m   \u001b[38;5;66;03m# Release the lock early so that multiple threads can perform the call\u001b[39;00m\n\u001b[0;32m    874\u001b[0m   \u001b[38;5;66;03m# in parallel.\u001b[39;00m\n\u001b[0;32m    875\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n",
      "File \u001b[1;32mc:\\users\\bbrel\\esa_webinar\\.venv\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\tracing_compilation.py:139\u001b[0m, in \u001b[0;36mcall_function\u001b[1;34m(args, kwargs, tracing_options)\u001b[0m\n\u001b[0;32m    137\u001b[0m bound_args \u001b[38;5;241m=\u001b[39m function\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39mbind(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    138\u001b[0m flat_inputs \u001b[38;5;241m=\u001b[39m function\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39munpack_inputs(bound_args)\n\u001b[1;32m--> 139\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_flat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# pylint: disable=protected-access\u001b[39;49;00m\n\u001b[0;32m    140\u001b[0m \u001b[43m    \u001b[49m\u001b[43mflat_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcaptured_inputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfunction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcaptured_inputs\u001b[49m\n\u001b[0;32m    141\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\users\\bbrel\\esa_webinar\\.venv\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\concrete_function.py:1322\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[1;34m(self, tensor_inputs, captured_inputs)\u001b[0m\n\u001b[0;32m   1318\u001b[0m possible_gradient_type \u001b[38;5;241m=\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[0;32m   1319\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (possible_gradient_type \u001b[38;5;241m==\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[0;32m   1320\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m executing_eagerly):\n\u001b[0;32m   1321\u001b[0m   \u001b[38;5;66;03m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[1;32m-> 1322\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_inference_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_preflattened\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1323\u001b[0m forward_backward \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[0;32m   1324\u001b[0m     args,\n\u001b[0;32m   1325\u001b[0m     possible_gradient_type,\n\u001b[0;32m   1326\u001b[0m     executing_eagerly)\n\u001b[0;32m   1327\u001b[0m forward_function, args_with_tangents \u001b[38;5;241m=\u001b[39m forward_backward\u001b[38;5;241m.\u001b[39mforward()\n",
      "File \u001b[1;32mc:\\users\\bbrel\\esa_webinar\\.venv\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\atomic_function.py:216\u001b[0m, in \u001b[0;36mAtomicFunction.call_preflattened\u001b[1;34m(self, args)\u001b[0m\n\u001b[0;32m    214\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcall_preflattened\u001b[39m(\u001b[38;5;28mself\u001b[39m, args: Sequence[core\u001b[38;5;241m.\u001b[39mTensor]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[0;32m    215\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"Calls with flattened tensor inputs and returns the structured output.\"\"\"\u001b[39;00m\n\u001b[1;32m--> 216\u001b[0m   flat_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_flat\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    217\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39mpack_output(flat_outputs)\n",
      "File \u001b[1;32mc:\\users\\bbrel\\esa_webinar\\.venv\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\atomic_function.py:251\u001b[0m, in \u001b[0;36mAtomicFunction.call_flat\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m    249\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m record\u001b[38;5;241m.\u001b[39mstop_recording():\n\u001b[0;32m    250\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_bound_context\u001b[38;5;241m.\u001b[39mexecuting_eagerly():\n\u001b[1;32m--> 251\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_bound_context\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    252\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    253\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    254\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunction_type\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mflat_outputs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    255\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    256\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    257\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m make_call_op_in_graph(\n\u001b[0;32m    258\u001b[0m         \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    259\u001b[0m         \u001b[38;5;28mlist\u001b[39m(args),\n\u001b[0;32m    260\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_bound_context\u001b[38;5;241m.\u001b[39mfunction_call_options\u001b[38;5;241m.\u001b[39mas_attrs(),\n\u001b[0;32m    261\u001b[0m     )\n",
      "File \u001b[1;32mc:\\users\\bbrel\\esa_webinar\\.venv\\lib\\site-packages\\tensorflow\\python\\eager\\context.py:1500\u001b[0m, in \u001b[0;36mContext.call_function\u001b[1;34m(self, name, tensor_inputs, num_outputs)\u001b[0m\n\u001b[0;32m   1498\u001b[0m cancellation_context \u001b[38;5;241m=\u001b[39m cancellation\u001b[38;5;241m.\u001b[39mcontext()\n\u001b[0;32m   1499\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cancellation_context \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 1500\u001b[0m   outputs \u001b[38;5;241m=\u001b[39m \u001b[43mexecute\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1501\u001b[0m \u001b[43m      \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mutf-8\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1502\u001b[0m \u001b[43m      \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1503\u001b[0m \u001b[43m      \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtensor_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1504\u001b[0m \u001b[43m      \u001b[49m\u001b[43mattrs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1505\u001b[0m \u001b[43m      \u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1506\u001b[0m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1507\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1508\u001b[0m   outputs \u001b[38;5;241m=\u001b[39m execute\u001b[38;5;241m.\u001b[39mexecute_with_cancellation(\n\u001b[0;32m   1509\u001b[0m       name\u001b[38;5;241m.\u001b[39mdecode(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m   1510\u001b[0m       num_outputs\u001b[38;5;241m=\u001b[39mnum_outputs,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1514\u001b[0m       cancellation_manager\u001b[38;5;241m=\u001b[39mcancellation_context,\n\u001b[0;32m   1515\u001b[0m   )\n",
      "File \u001b[1;32mc:\\users\\bbrel\\esa_webinar\\.venv\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py:53\u001b[0m, in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     51\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     52\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[1;32m---> 53\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m \u001b[43mpywrap_tfe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTFE_Py_Execute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_handle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     54\u001b[0m \u001b[43m                                      \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     55\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     56\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "''' import tensorflow_datasets as tfds\n",
    "\n",
    "# Load and preprocess the EuroSAT dataset\n",
    "def preprocess_image(image, label):\n",
    "    image = tf.image.resize(image, [32, 32])  # Resize to 32x32\n",
    "    image = (image - 127.5) / 127.5  # Normalize to [-1, 1]\n",
    "    return image, label\n",
    "\n",
    "# Load EuroSAT dataset\n",
    "dataset, info = tfds.load('eurosat', split='train', with_info=True, as_supervised=True)\n",
    "dataset = dataset.map(preprocess_image)\n",
    "\n",
    "# Define the generator model\n",
    "def build_generator():\n",
    "    model = tf.keras.Sequential()\n",
    "    model.add(layers.Input(shape=(100,)))  # Define the input shape using Input layer\n",
    "    model.add(layers.Dense(256))\n",
    "    model.add(layers.LeakyReLU(negative_slope=0.2))  # Use negative_slope instead of alpha\n",
    "    model.add(layers.BatchNormalization(momentum=0.8))\n",
    "    model.add(layers.Dense(512))\n",
    "    model.add(layers.LeakyReLU(negative_slope=0.2))\n",
    "    model.add(layers.BatchNormalization(momentum=0.8))\n",
    "    model.add(layers.Dense(1024))\n",
    "    model.add(layers.LeakyReLU(negative_slope=0.2))\n",
    "    model.add(layers.BatchNormalization(momentum=0.8))\n",
    "    model.add(layers.Dense(np.prod((32, 32, 3)), activation='tanh'))\n",
    "    model.add(layers.Reshape((32, 32, 3)))\n",
    "    return model\n",
    "\n",
    "# Define the discriminator model\n",
    "def build_discriminator():\n",
    "    model = tf.keras.Sequential()\n",
    "    model.add(layers.Input(shape=(32, 32, 3)))  # Define the input shape using Input layer\n",
    "    model.add(layers.Flatten())\n",
    "    model.add(layers.Dense(512))\n",
    "    model.add(layers.LeakyReLU(negative_slope=0.2))\n",
    "    model.add(layers.Dense(256))\n",
    "    model.add(layers.LeakyReLU(negative_slope=0.2))\n",
    "    model.add(layers.Dense(1, activation='sigmoid'))\n",
    "    return model\n",
    "\n",
    "# Compile the models\n",
    "generator = build_generator()\n",
    "discriminator = build_discriminator()\n",
    "\n",
    "# Optimizers\n",
    "generator_optimizer = tf.keras.optimizers.Adam(0.0002, 0.5)\n",
    "discriminator_optimizer = tf.keras.optimizers.Adam(0.0002, 0.5)\n",
    "\n",
    "# Loss function\n",
    "cross_entropy = tf.keras.losses.BinaryCrossentropy(from_logits=False)\n",
    "\n",
    "# Training step function\n",
    "@tf.function\n",
    "def train_step(real_images):\n",
    "    noise = tf.random.normal([batch_size, 100])\n",
    "    fake_images = generator(noise, training=True)\n",
    "\n",
    "    with tf.GradientTape() as disc_tape:\n",
    "        real_output = discriminator(real_images, training=True)\n",
    "        fake_output = discriminator(fake_images, training=True)\n",
    "        disc_loss_real = cross_entropy(tf.ones_like(real_output), real_output)\n",
    "        disc_loss_fake = cross_entropy(tf.zeros_like(fake_output), fake_output)\n",
    "        disc_loss = disc_loss_real + disc_loss_fake\n",
    "\n",
    "    gradients_of_discriminator = disc_tape.gradient(disc_loss, discriminator.trainable_variables)\n",
    "    discriminator_optimizer.apply_gradients(zip(gradients_of_discriminator, discriminator.trainable_variables))\n",
    "\n",
    "    noise = tf.random.normal([batch_size, 100])\n",
    "    with tf.GradientTape() as gen_tape:\n",
    "        generated_images = generator(noise, training=True)\n",
    "        gen_output = discriminator(generated_images, training=False)\n",
    "        gen_loss = cross_entropy(tf.ones_like(gen_output), gen_output)\n",
    "\n",
    "    gradients_of_generator = gen_tape.gradient(gen_loss, generator.trainable_variables)\n",
    "    generator_optimizer.apply_gradients(zip(gradients_of_generator, generator.trainable_variables))\n",
    "\n",
    "    return disc_loss, gen_loss\n",
    "\n",
    "# Function to train the GAN\n",
    "def train_gan(epochs, batch_size, sample_interval):\n",
    "    X_train = []\n",
    "    for image, label in tfds.as_numpy(dataset):\n",
    "        image = np.array(image)\n",
    "        if image.shape == (32, 32, 3):  # Ensure the shape is as expected\n",
    "            X_train.append(image)\n",
    "        else:\n",
    "            print(f\"Skipping image with shape {image.shape}\")  # Debug statement\n",
    "    X_train = np.array(X_train)\n",
    "\n",
    "    print(f\"Total images loaded: {len(X_train)}\")\n",
    "    if len(X_train) == 0:\n",
    "        print(\"No images were loaded. Please check the dataset and preprocessing steps.\")\n",
    "        return\n",
    "\n",
    "    half_batch = int(batch_size / 2)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        for i in range(len(X_train) // batch_size):\n",
    "            real_images = X_train[i * batch_size:(i + 1) * batch_size]\n",
    "            d_loss, g_loss = train_step(real_images)\n",
    "\n",
    "        if epoch % sample_interval == 0:\n",
    "            print(f\"{epoch} [D loss: {d_loss.numpy()} | G loss: {g_loss.numpy()}]\")\n",
    "\n",
    "# Function to generate and display images\n",
    "def generate_images(generator, num_images):\n",
    "    noise = np.random.normal(0, 1, (num_images, 100))\n",
    "    gen_images = generator.predict(noise)\n",
    "    gen_images = 0.5 * gen_images + 0.5  # Rescale to [0, 1]\n",
    "    return gen_images\n",
    "\n",
    "def display_generated_images(generator, epoch, num_images=10):\n",
    "    gen_images = generate_images(generator, num_images)\n",
    "    plt.figure(figsize=(10, 10))\n",
    "    for i in range(num_images):\n",
    "        plt.subplot(5, 2, i+1)\n",
    "        plt.imshow(gen_images[i])\n",
    "        plt.axis('off')\n",
    "    plt.suptitle(f\"Generated Images at Epoch {epoch}\")\n",
    "    plt.show()\n",
    "\n",
    "# Set batch size\n",
    "batch_size = 64\n",
    "\n",
    "# Train the GAN\n",
    "train_gan(epochs=1000, batch_size=batch_size, sample_interval=100)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc1bb579-3df7-4e52-8541-1339752d553d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the generator model\n",
    "def build_generator() -> models.Sequential:\n",
    "    \"\"\"\n",
    "    Builds the generator model for the GAN.\n",
    "\n",
    "    Returns:\n",
    "    models.Sequential: The generator model.\n",
    "    \"\"\"\n",
    "    model = models.Sequential()\n",
    "    model.add(layers.Dense(256, input_dim=100, activation='relu'))\n",
    "    model.add(layers.BatchNormalization())\n",
    "    model.add(layers.LeakyReLU(alpha=0.2))\n",
    "    model.add(layers.Dense(512, activation='relu'))\n",
    "    model.add(layers.BatchNormalization())\n",
    "    model`b.add(layers.LeakyReLU(alpha=0.2))\n",
    "    model.add(layers.Dense(1024, activation='relu'))\n",
    "    model.add(layers.BatchNormalization())\n",
    "    model.add(layers.LeakyReLU(alpha=0.2))\n",
    "    model.add(layers.Dense(28 * 28 * 1, activation='tanh'))\n",
    "    model.add(layers.Reshape((28, 28, 1)))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21e3f696-adf9-45e4-93fb-87316c722d11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the discriminator model\n",
    "def build_discriminator() -> models.Sequential:\n",
    "    \"\"\"\n",
    "    Builds the discriminator model for the GAN.\n",
    "\n",
    "    Returns:\n",
    "    models.Sequential: The discriminator model.\n",
    "    \"\"\"\n",
    "    model = models.Sequential()\n",
    "    model.add(layers.Flatten(input_shape=(28, 28, 1)))\n",
    "    model.add(layers.Dense(512, activation='relu'))\n",
    "    model.add(layers.LeakyReLU(alpha=0.2))\n",
    "    model.add(layers.Dense(256, activation='relu'))\n",
    "    model.add(layers.LeakyReLU(alpha=0.2))\n",
    "    model.add(layers.Dense(1, activation='sigmoid'))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c98b6965-e549-4d33-9b4b-ed4b4c4dbb6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the discriminator\n",
    "discriminator: models.Sequential = build_discriminator()\n",
    "discriminator.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "011fd44d-3949-4f1b-a9b1-0d3638c34432",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the generator\n",
    "generator: models.Sequential = build_generator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb721c0c-cccf-4dba-a986-feaf62b5d4e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the combined GAN model (stacked generator and discriminator)\n",
    "def build_gan(generator: models.Sequential, discriminator: models.Sequential) -> models.Sequential:\n",
    "    \"\"\"\n",
    "    Builds the combined GAN model by stacking the generator and the discriminator.\n",
    "\n",
    "    Args:\n",
    "    generator (models.Sequential): The generator model.\n",
    "    discriminator (models.Sequential): The discriminator model.\n",
    "\n",
    "    Returns:\n",
    "    models.Sequential: The combined GAN model.\n",
    "    \"\"\"\n",
    "    discriminator.trainable = False\n",
    "    model = models.Sequential()\n",
    "    model.add(generator)\n",
    "    model.add(discriminator)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73e84e2d-9aaa-4255-bff7-a861782ecbb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the GAN\n",
    "gan: models.Sequential = build_gan(generator, discriminator)\n",
    "gan.compile(optimizer='adam', loss='binary_crossentropy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22ea2a5e-465b-4ab7-aad4-610513d95c83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training the GAN\n",
    "def train_gan(gan: models.Sequential, generator: models.Sequential, discriminator: models.Sequential, \n",
    "              epochs: int, batch_size: int, training_data: np.ndarray) -> None:\n",
    "    \"\"\"\n",
    "    Trains the GAN model.\n",
    "\n",
    "    Args:\n",
    "    gan (models.Sequential): The combined GAN model.\n",
    "    generator (models.Sequential): The generator model.\n",
    "    discriminator (models.Sequential): The discriminator model.\n",
    "    epochs (int): Number of training epochs.\n",
    "    batch_size (int): Size of the training batch.\n",
    "    training_data (np.ndarray): Training dataset.\n",
    "    \"\"\"\n",
    "    half_batch = int(batch_size / 2)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        # Train the discriminator with real samples\n",
    "        idx = np.random.randint(0, training_data.shape[0], half_batch)\n",
    "        real_images = training_data[idx]\n",
    "        real_labels = np.ones((half_batch, 1))\n",
    "        d_loss_real = discriminator.train_on_batch(real_images, real_labels)\n",
    "\n",
    "        # Train the discriminator with fake samples\n",
    "        noise = np.random.normal(0, 1, (half_batch, 100))\n",
    "        fake_images = generator.predict(noise)\n",
    "        fake_labels = np.zeros((half_batch, 1))\n",
    "        d_loss_fake = discriminator.train_on_batch(fake_images, fake_labels)\n",
    "\n",
    "        # Train the GAN\n",
    "        noise = np.random.normal(0, 1, (batch_size, 100))\n",
    "        valid_labels = np.ones((batch_size, 1))\n",
    "        g_loss = gan.train_on_batch(noise, valid_labels)\n",
    "\n",
    "        # Print the progress\n",
    "        print(f\"{epoch + 1}/{epochs} [D loss: {0.5 * (d_loss_real[0] + d_loss_fake[0]):.4f}, acc.: {100 * 0.5 * (d_loss_real[1] + d_loss_fake[1]):.2f}%] [G loss: {g_loss:.4f}]\")\n",
    "\n",
    "# Assume training_data is a preprocessed dataset of images\n",
    "# train_gan(gan, generator, discriminator, epochs=10000, batch_size=64, training_data=training_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f264ff4c-b713-41aa-af4c-2c499bfe21d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generating and displaying images\n",
    "def generate_and_display_images(generator: models.Sequential, num_images: int = 5) -> None:\n",
    "    \"\"\"\n",
    "    Generates and displays images using the trained generator model.\n",
    "\n",
    "    Args:\n",
    "    generator (models.Sequential): The trained generator model.\n",
    "    num_images (int): Number of images to generate and display.\n",
    "    \"\"\"\n",
    "    noise = np.random.normal(0, 1, (num_images, 100))\n",
    "    generated_images = generator.predict(noise)\n",
    "\n",
    "    for i in range(num_images):\n",
    "        plt.imshow(generated_images[i, :, :, 0], cmap='gray')\n",
    "        plt.title('Generated Image')\n",
    "        plt.axis('off')\n",
    "        plt.show()\n",
    "\n",
    "# generate_and_display_images(generator, num_images=5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb60b5a7-71b3-41f8-8c17-3de53fbd6c2b",
   "metadata": {},
   "source": [
    "## Variational Autoencoder (VAE)\n",
    "\n",
    "This section demonstrates how to create, compile, train, and generate images using a Variational Autoencoder (VAE). VAEs are generative models that learn to encode input data into a latent space and decode from the latent space to generate new data.\n",
    "\n",
    "### Steps:\n",
    "1. **Create the encoder and decoder models.**\n",
    "2. **Define the VAE by combining the encoder and decoder.**\n",
    "3. **Compile the VAE with an appropriate loss function.**\n",
    "4. **Train the VAE on a dataset of images.**\n",
    "5. **Generate and display new images using the trained decoder.**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d25d2db-a2cd-4a8d-9478-001175da9bf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import backend as K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "160b0676-738a-4377-a644-92da27ab1227",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sampling function for the latent space\n",
    "def sampling(args: Tuple[tf.Tensor, tf.Tensor]) -> tf.Tensor:\n",
    "    \"\"\"\n",
    "    Reparameterization trick by sampling from an isotropic unit Gaussian.\n",
    "\n",
    "    Args:\n",
    "    args (Tuple[tf.Tensor, tf.Tensor]): Mean and log of variance of the latent space.\n",
    "\n",
    "    Returns:\n",
    "    tf.Tensor: Sampled latent vector.\n",
    "    \"\"\"\n",
    "    z_mean, z_log_var = args\n",
    "    batch = tf.shape(z_mean)[0]\n",
    "    dim = tf.shape(z_mean)[1]\n",
    "    epsilon = K.random_normal(shape=(batch, dim))\n",
    "    return z_mean + K.exp(0.5 * z_log_var) * epsilon"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0140d818-f9a5-4f00-baa4-90fdf5b0abcd",
   "metadata": {},
   "source": [
    "#### Components and Steps for the sampling method.\n",
    "\n",
    "1. **Inputs (args: Tuple[tf.Tensor, tf.Tensor])**\n",
    "   - `z_mean`: The mean of the latent space distribution (output of the encoder).\n",
    "   - `z_log_var`: The logarithm of the variance of the latent space distribution (output of the encoder).\n",
    "\n",
    "2. **Compute Batch and Dimension**\n",
    "   - `batch`: The number of samples in the batch (i.e., the batch size).\n",
    "   - `dim`: The dimensionality of the latent space (latent dimension).\n",
    "\n",
    "3. **Generate Random Noise (epsilon)**\n",
    "   - `epsilon`: A tensor of random values sampled from a standard normal distribution (mean = 0, standard deviation = 1) with the same shape as the latent space (batch size, latent dimension).\n",
    "\n",
    "4. **Reparameterization Trick**\n",
    "   - `z = z_mean + K.exp(0.5 * z_log_var) * epsilon`: This line applies the reparameterization trick to generate a sample `z` from the latent space. Here's a breakdown of the operations:\n",
    "     - `K.exp(0.5 * z_log_var)`: Converts the log variance to the standard deviation by exponentiating it and taking the square root (since `exp(0.5 * log_var) = sqrt(exp(log_var))`).\n",
    "     - `K.exp(0.5 * z_log_var) * epsilon`: Scales the random noise `epsilon` by the standard deviation.\n",
    "     - `z_mean + ...`: Shifts the scaled noise by the mean to produce the final sample `z`.\n",
    "\n",
    "\n",
    "##### Why is the Reparameterization Trick Necessary?\n",
    "In a VAE, the goal is to sample from a learned distribution (characterized by z_mean and z_log_var) during training. Directly sampling from this distribution would make it challenging to propagate gradients through the sampling operation because it introduces non-differentiable randomness.\n",
    "\n",
    "The reparameterization trick solves this by expressing the random sample z as a deterministic function of z_mean, z_log_var, and a random variable epsilon drawn from a standard normal distribution. This formulation allows gradients to be backpropagated through z_mean and z_log_var, enabling the use of gradient-based optimization techniques.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dc3ba92-f752-47e4-b37c-add8842fb385",
   "metadata": {},
   "source": [
    "### Understanding the Reparameterization Trick in Layman's Terms\n",
    "\n",
    "The reparameterization trick is a clever method used in machine learning to make a complicated process easier to manage. It's especially important in Variational Autoencoders (VAEs), which are models that learn to generate new data, like creating new images that look like they came from a given dataset.\n",
    "\n",
    "### The Problem\n",
    "\n",
    "In a VAE, we have two parts: an encoder and a decoder.\n",
    "\n",
    "1. **Encoder:** Takes an input (like an image) and compresses it into a smaller, simpler form called a \"latent space.\" This latent space is described by two things:\n",
    "   - **Mean (`z_mean`)**: The central point of this compressed space.\n",
    "   - **Log Variance (`z_log_var`)**: Describes the spread or uncertainty around the mean.\n",
    "\n",
    "2. **Decoder:** Takes this compressed form from the latent space and tries to reconstruct the original input (like recreating the image).\n",
    "\n",
    "To make the model work, we need to sample points from this latent space. However, this sampling step is random and makes it hard to adjust the model during training because we can't easily calculate gradients, which are essential for learning.\n",
    "\n",
    "### The Reparameterization Trick\n",
    "\n",
    "The reparameterization trick is like a recipe for baking a cake. Instead of baking a cake directly, which might be tricky, you break it down into simpler steps that are easier to follow.\n",
    "\n",
    "1. **Break Down the Sampling:**\n",
    "   - Normally, sampling directly from the latent space would be hard.\n",
    "   - Instead, we separate the process into simpler parts.\n",
    "\n",
    "2. **Add Randomness:**\n",
    "   - We start with a simple random noise (`epsilon`), like the basic ingredients in baking.\n",
    "   - This noise is easy to handle because it's just random numbers from a normal distribution (like the basic ingredients you always have in your kitchen).\n",
    "\n",
    "3. **Transform the Noise:**\n",
    "   - We mix this noise with our \"mean\" and \"variance\" to get our final sample (`z`).\n",
    "   - This is like mixing your ingredients according to a recipe to get the cake batter.\n",
    "\n",
    "Heres how the steps look in simpler terms:\n",
    "- **Start with the mean (`z_mean`):** This is our central point.\n",
    "- **Adjust the spread using the log variance (`z_log_var`):** This tells us how much to spread out from the mean.\n",
    "- **Add randomness (`epsilon`):** This is our simple random noise.\n",
    "- **Combine them:** Mix the mean, the adjusted spread, and the noise to get our final sample.\n",
    "\n",
    "### Why This Trick is Useful\n",
    "\n",
    "By breaking down the sampling process, we can:\n",
    "- **Keep Things Simple:** The trick makes the sampling process easier to manage and understand.\n",
    "- **Allow Learning:** Even though there's randomness, the process remains smooth enough for the model to learn and improve.\n",
    "- **Train Efficiently:** We can use standard techniques to train the model, making the whole system more effective and stable.\n",
    "\n",
    "### Real-World Analogy\n",
    "\n",
    "Imagine you're trying to learn to draw circles of different sizes:\n",
    "- **Direct Method:** You try to draw circles of random sizes directly, which can be chaotic and hard to improve upon.\n",
    "- **Reparameterization Trick:** Instead, you first draw a standard-sized circle and then use a ruler (mean) and some guidelines (variance) to scale it up or down, adding a bit of randomness. This way, you can practice and improve more systematically.\n",
    "\n",
    "### Summary\n",
    "\n",
    "The reparameterization trick is a smart way to simplify a complex, random process in machine learning. It allows models like VAEs to learn and improve efficiently by breaking down the hard part (sampling) into simpler, manageable steps. This trick ensures that the model can generate new data effectively while still being easy to train and adjust.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65e7c540-7448-401d-8ed1-b431f8c5f13c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the encoder model\n",
    "def build_encoder(input_shape: Tuple[int, int, int], latent_dim: int) -> models.Model:\n",
    "    \"\"\"\n",
    "    Builds the encoder model for the VAE.\n",
    "\n",
    "    Args:\n",
    "    input_shape (Tuple[int, int, int]): Shape of the input images.\n",
    "    latent_dim (int): Dimensionality of the latent space.\n",
    "\n",
    "    Returns:\n",
    "    models.Model: The encoder model.\n",
    "    \"\"\"\n",
    "    inputs = layers.Input(shape=input_shape)\n",
    "    x = layers.Conv2D(32, (3, 3), activation='relu', padding='same')(inputs)\n",
    "    x = layers.MaxPooling2D((2, 2))(x)\n",
    "    x = layers.Conv2D(64, (3, 3), activation='relu', padding='same')(x)\n",
    "    x = layers.MaxPooling2D((2, 2))(x)\n",
    "    x = layers.Conv2D(128, (3, 3), activation='relu', padding='same')(x)\n",
    "    x = layers.Flatten()(x)\n",
    "    x = layers.Dense(128, activation='relu')(x)\n",
    "    z_mean = layers.Dense(latent_dim)(x)\n",
    "    z_log_var = layers.Dense(latent_dim)(x)\n",
    "    z = layers.Lambda(sampling, output_shape=(latent_dim,), name='z')([z_mean, z_log_var])\n",
    "    encoder = models.Model(inputs, [z_mean, z_log_var, z], name='encoder')\n",
    "    return encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c79c13eb-5bff-4f99-a691-c54df3ff5854",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the decoder model\n",
    "def build_decoder(latent_dim: int, output_shape: Tuple[int, int, int]) -> models.Model:\n",
    "    \"\"\"\n",
    "    Builds the decoder model for the VAE.\n",
    "\n",
    "    Args:\n",
    "    latent_dim (int): Dimensionality of the latent space.\n",
    "    output_shape (Tuple[int, int, int]): Shape of the output images.\n",
    "\n",
    "    Returns:\n",
    "    models.Model: The decoder model.\n",
    "    \"\"\"\n",
    "    latent_inputs = layers.Input(shape=(latent_dim,))\n",
    "    x = layers.Dense(128, activation='relu')(latent_inputs)\n",
    "    x = layers.Dense(np.prod(output_shape), activation='relu')(x)\n",
    "    x = layers.Reshape(output_shape)(x)\n",
    "    x = layers.Conv2DTranspose(128, (3, 3), activation='relu', padding='same')(x)\n",
    "    x = layers.UpSampling2D((2, 2))(x)\n",
    "    x = layers.Conv2DTranspose(64, (3, 3), activation='relu', padding='same')(x)\n",
    "    x = layers.UpSampling2D((2, 2))(x)\n",
    "    outputs = layers.Conv2DTranspose(output_shape[-1], (3, 3), activation='sigmoid', padding='same')(x)\n",
    "    decoder = models.Model(latent_inputs, outputs, name='decoder')\n",
    "    return decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1e95014-cae3-4626-b447-49af5513503b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and preprocess the EuroSAT dataset\n",
    "def preprocess_image(image, label):\n",
    "    image = tf.image.resize(image, [64, 64])  # Resize to 64x64\n",
    "    image = (image - 127.5) / 127.5  # Normalize to [-1, 1]\n",
    "    return image, image  # Using image as both input and output for reconstruction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a460830f-b755-4d38-a44d-701cae945767",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tfds' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Load EuroSAT dataset\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m dataset, info \u001b[38;5;241m=\u001b[39m \u001b[43mtfds\u001b[49m\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124meurosat\u001b[39m\u001b[38;5;124m'\u001b[39m, split\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m'\u001b[39m, with_info\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, as_supervised\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m      3\u001b[0m dataset \u001b[38;5;241m=\u001b[39m dataset\u001b[38;5;241m.\u001b[39mmap(preprocess_image)\u001b[38;5;241m.\u001b[39mbatch(\u001b[38;5;241m64\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'tfds' is not defined"
     ]
    }
   ],
   "source": [
    "# Load EuroSAT dataset\n",
    "dataset, info = tfds.load('eurosat', split='train', with_info=True, as_supervised=True)\n",
    "dataset = dataset.map(preprocess_image).batch(64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c26bfb94-81dc-40ae-948e-763673b49b22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the VAE model\n",
    "def build_vae(encoder: models.Model, decoder: models.Model) -> models.Model:\n",
    "    \"\"\"\n",
    "    Builds the Variational Autoencoder (VAE) model by combining the encoder and the decoder.\n",
    "\n",
    "    Args:\n",
    "    encoder (models.Model): The encoder model.\n",
    "    decoder (models.Model): The decoder model.\n",
    "\n",
    "    Returns:\n",
    "    models.Model: The VAE model.\n",
    "    \"\"\"\n",
    "    inputs = encoder.input\n",
    "    z_mean, z_log_var, z = encoder(inputs)\n",
    "    reconstructed = decoder(z)\n",
    "    vae = models.Model(inputs, reconstructed, name='vae')\n",
    "\n",
    "    # Define VAE loss\n",
    "    reconstruction_loss = tf.keras.losses.binary_crossentropy(K.flatten(inputs), K.flatten(reconstructed))\n",
    "    reconstruction_loss *= np.prod(input_shape)\n",
    "    kl_loss = 1 + z_log_var - K.square(z_mean) - K.exp(z_log_var)\n",
    "    kl_loss = K.sum(kl_loss, axis=-1)\n",
    "    kl_loss *= -0.5\n",
    "    vae_loss = K.mean(reconstruction_loss + kl_loss)\n",
    "    vae.add_loss(vae_loss)\n",
    "\n",
    "    return vae"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76f11fff-a2b5-4b37-9cf8-3b6d415c2ab8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define input and latent dimensions\n",
    "input_shape = (28, 28, 1)\n",
    "latent_dim = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cbcab2f-a1da-49da-9d55-f0cda833cad6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the encoder and decoder\n",
    "encoder: models.Model = build_encoder(input_shape, latent_dim)\n",
    "decoder: models.Model = build_decoder(latent_dim, input_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74e0dcd7-ee75-4e38-9d4a-f4af2405f906",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build and compile the VAE\n",
    "vae: models.Model = build_vae(encoder, decoder)\n",
    "vae.compile(optimizer='adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ffeeede-8fcc-4d0c-81fd-fdaedcd4af43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training the VAE\n",
    "def train_vae(vae: models.Model, training_data: np.ndarray, epochs: int, batch_size: int) -> tf.keras.callbacks.History:\n",
    "    \"\"\"\n",
    "    Trains the VAE model.\n",
    "\n",
    "    Args:\n",
    "    vae (models.Model): The VAE model.\n",
    "    training_data (np.ndarray): Training dataset.\n",
    "    epochs (int): Number of training epochs.\n",
    "    batch_size (int): Size of the training batch.\n",
    "\n",
    "    Returns:\n",
    "    tf.keras.callbacks.History: History object containing training history.\n",
    "    \"\"\"\n",
    "    history = vae.fit(training_data, training_data, epochs=epochs, batch_size=batch_size, validation_split=0.1)\n",
    "    return history\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd9929e5-8499-4253-9b8f-1c3dc7d62086",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assume training_data is a preprocessed dataset of images\n",
    "history = train_vae(vae, training_data, epochs=50, batch_size=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8487e1e-18e5-4734-821f-5215b32ae77c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generating and displaying images\n",
    "def generate_and_display_images(decoder: models.Model, num_images: int = 5) -> None:\n",
    "    \"\"\"\n",
    "    Generates and displays images using the trained decoder model.\n",
    "\n",
    "    Args:\n",
    "    decoder (models.Model): The trained decoder model.\n",
    "    num_images (int): Number of images to generate and display.\n",
    "    \"\"\"\n",
    "    grid_x = np.linspace(-2, 2, num_images)\n",
    "    grid_y = np.linspace(-2, 2, num_images)\n",
    "\n",
    "    for i, yi in enumerate(grid_x):\n",
    "        for j, xi in enumerate(grid_y):\n",
    "            z_sample = np.array([[xi, yi]])\n",
    "            generated_image = decoder.predict(z_sample)\n",
    "            plt.imshow(generated_image[0, :, :, 0], cmap='gray')\n",
    "            plt.title('Generated Image')\n",
    "            plt.axis('off')\n",
    "            plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d5a63f8-6e12-4896-8233-a804e57c8cdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_and_display_images(decoder, num_images=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e416f3d-c578-4876-a402-60d7492a4fc7",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "'''\n",
    "# Define the encoder model\n",
    "def build_encoder(latent_dim):\n",
    "    encoder_input = layers.Input(shape=(64, 64, 3))\n",
    "    x = layers.Conv2D(32, (3, 3), activation='relu', padding='same')(encoder_input)\n",
    "    x = layers.MaxPooling2D((2, 2), padding='same')(x)\n",
    "    x = layers.Conv2D(64, (3, 3), activation='relu', padding='same')(x)\n",
    "    x = layers.MaxPooling2D((2, 2), padding='same')(x)\n",
    "    x = layers.Conv2D(128, (3, 3), activation='relu', padding='same')(x)\n",
    "    x = layers.MaxPooling2D((2, 2), padding='same')(x)\n",
    "    x = layers.Flatten()(x)\n",
    "    x = layers.Dense(256, activation='relu')(x)\n",
    "    z_mean = layers.Dense(latent_dim)(x)\n",
    "    z_log_var = layers.Dense(latent_dim)(x)\n",
    "    return models.Model(encoder_input, [z_mean, z_log_var], name='encoder')\n",
    "\n",
    "# Define the decoder model\n",
    "def build_decoder(latent_dim):\n",
    "    decoder_input = layers.Input(shape=(latent_dim,))\n",
    "    x = layers.Dense(8*8*128, activation='relu')(decoder_input)\n",
    "    x = layers.Reshape((8, 8, 128))(x)\n",
    "    x = layers.Conv2DTranspose(128, (3, 3), activation='relu', padding='same')(x)\n",
    "    x = layers.UpSampling2D((2, 2))(x)\n",
    "    x = layers.Conv2DTranspose(64, (3, 3), activation='relu', padding='same')(x)\n",
    "    x = layers.UpSampling2D((2, 2))(x)\n",
    "    x = layers.Conv2DTranspose(32, (3, 3), activation='relu', padding='same')(x)\n",
    "    x = layers.UpSampling2D((2, 2))(x)\n",
    "    decoder_output = layers.Conv2DTranspose(3, (3, 3), activation='tanh', padding='same')(x)\n",
    "    return models.Model(decoder_input, decoder_output, name='decoder')\n",
    "\n",
    "# Define the VAE model\n",
    "class VAE(tf.keras.Model):\n",
    "    def __init__(self, encoder, decoder, **kwargs):\n",
    "        super(VAE, self).__init__(**kwargs)\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "\n",
    "    def call(self, inputs):\n",
    "        z_mean, z_log_var = self.encoder(inputs)\n",
    "        epsilon = K.random_normal(shape=K.shape(z_mean))\n",
    "        z = z_mean + K.exp(0.5 * z_log_var) * epsilon\n",
    "        reconstructed = self.decoder(z)\n",
    "        kl_loss = -0.5 * K.mean(1 + z_log_var - K.square(z_mean) - K.exp(z_log_var))\n",
    "        self.add_loss(kl_loss)\n",
    "        return reconstructed\n",
    "\n",
    "# Hyperparameters\n",
    "latent_dim = 64\n",
    "\n",
    "# Build encoder and decoder\n",
    "encoder = build_encoder(latent_dim)\n",
    "decoder = build_decoder(latent_dim)\n",
    "\n",
    "# Build and compile VAE\n",
    "vae = VAE(encoder, decoder)\n",
    "vae.compile(optimizer='adam', loss='mean_squared_error')\n",
    "\n",
    "# Custom training loop to handle the custom loss\n",
    "epochs = 50\n",
    "for epoch in range(epochs):\n",
    "    for step, (batch, _) in enumerate(dataset):\n",
    "        with tf.GradientTape() as tape:\n",
    "            reconstruction = vae(batch)\n",
    "            reconstruction_loss = tf.reduce_mean(tf.square(batch - reconstruction))\n",
    "            total_loss = reconstruction_loss + sum(vae.losses)\n",
    "        gradients = tape.gradient(total_loss, vae.trainable_variables)\n",
    "        vae.optimizer.apply_gradients(zip(gradients, vae.trainable_variables))\n",
    "        if step % 100 == 0:\n",
    "            print(f\"Epoch {epoch + 1}, Step {step}, Loss: {total_loss.numpy()}\")\n",
    "\n",
    "# Generate new images\n",
    "def generate_images(num_images):\n",
    "    z_sample = np.random.normal(size=(num_images, latent_dim))\n",
    "    generated_images = decoder.predict(z_sample)\n",
    "    generated_images = 0.5 * generated_images + 0.5  # Rescale to [0, 1]\n",
    "    return generated_images\n",
    "\n",
    "# Display generated images\n",
    "def display_generated_images(num_images=10):\n",
    "    generated_images = generate_images(num_images)\n",
    "    plt.figure(figsize=(10, 10))\n",
    "    for i in range(num_images):\n",
    "        plt.subplot(5, 2, i+1)\n",
    "        plt.imshow(generated_images[i])\n",
    "        plt.axis('off')\n",
    "    plt.show()\n",
    "\n",
    "# Display generated images\n",
    "display_generated_images()\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9e171256-52ac-4317-8b30-fea326edc8a5",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (3588527104.py, line 3)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[3], line 3\u001b[1;36m\u001b[0m\n\u001b[1;33m    This section demonstrates how to create, compile, and train a U-Net model. U-Net is a convolutional network architecture for fast and precise segmentation of images. It consists of a contracting path to capture context and a symmetric expanding path that enables precise localization.\u001b[0m\n\u001b[1;37m         ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "## U-Net Model\n",
    "\n",
    "This section demonstrates how to create, compile, and train a U-Net model.\n",
    "U-Net is a convolutional network architecture for fast and precise segmentation of images.\n",
    "It consists of a contracting path to capture context and a symmetric expanding path that \n",
    "enables precise localization.\n",
    "\n",
    "### Steps:\n",
    "1. **Create the U-Net model.**\n",
    "2. **Compile the model with an appropriate optimizer and loss function.**\n",
    "3. **Train the model on a dataset.**\n",
    "4. **Evaluate the model's performance.**\n",
    "5. **Visualize the model's predictions.**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fad6990-2549-40bb-aacc-d907d69686f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to create a U-Net model\n",
    "def unet_model(input_shape: Tuple[int, int, int]) -> models.Model:\n",
    "    \"\"\"\n",
    "    Creates a U-Net model for image segmentation.\n",
    "\n",
    "    Args:\n",
    "    input_shape (Tuple[int, int, int]): Shape of the input images.\n",
    "\n",
    "    Returns:\n",
    "    models.Model: The U-Net model.\n",
    "    \"\"\"\n",
    "    inputs = layers.Input(shape=input_shape)\n",
    "    \n",
    "    # Contracting path\n",
    "    c1 = layers.Conv2D(64, (3, 3), activation='relu', padding='same')(inputs)\n",
    "    c1 = layers.Conv2D(64, (3, 3), activation='relu', padding='same')(c1)\n",
    "    p1 = layers.MaxPooling2D((2, 2))(c1)\n",
    "\n",
    "    c2 = layers.Conv2D(128, (3, 3), activation='relu', padding='same')(p1)\n",
    "    c2 = layers.Conv2D(128, (3, 3), activation='relu', padding='same')(c2)\n",
    "    p2 = layers.MaxPooling2D((2, 2))(c2)\n",
    "\n",
    "    c3 = layers.Conv2D(256, (3, 3), activation='relu', padding='same')(p2)\n",
    "    c3 = layers.Conv2D(256, (3, 3), activation='relu', padding='same')(c3)\n",
    "    p3 = layers.MaxPooling2D((2, 2))(c3)\n",
    "\n",
    "    c4 = layers.Conv2D(512, (3, 3), activation='relu', padding='same')(p3)\n",
    "    c4 = layers.Conv2D(512, (3, 3), activation='relu', padding='same')(c4)\n",
    "    p4 = layers.MaxPooling2D((2, 2))(c4)\n",
    "\n",
    "    c5 = layers.Conv2D(1024, (3, 3), activation='relu', padding='same')(p4)\n",
    "    c5 = layers.Conv2D(1024, (3, 3), activation='relu', padding='same')(c5)\n",
    "\n",
    "    # Expanding path\n",
    "    u6 = layers.Conv2DTranspose(512, (2, 2), strides=(2, 2), padding='same')(c5)\n",
    "    u6 = layers.concatenate([u6, c4])\n",
    "    c6 = layers.Conv2D(512, (3, 3), activation='relu', padding='same')(u6)\n",
    "    c6 = layers.Conv2D(512, (3, 3), activation='relu', padding='same')(c6)\n",
    "\n",
    "    u7 = layers.Conv2DTranspose(256, (2, 2), strides=(2, 2), padding='same')(c6)\n",
    "    u7 = layers.concatenate([u7, c3])\n",
    "    c7 = layers.Conv2D(256, (3, 3), activation='relu', padding='same')(u7)\n",
    "    c7 = layers.Conv2D(256, (3, 3), activation='relu', padding='same')(c7)\n",
    "\n",
    "    u8 = layers.Conv2DTranspose(128, (2, 2), strides=(2, 2), padding='same')(c7)\n",
    "    u8 = layers.concatenate([u8, c2])\n",
    "    c8 = layers.Conv2D(128, (3, 3), activation='relu', padding='same')(u8)\n",
    "    c8 = layers.Conv2D(128, (3, 3), activation='relu', padding='same')(c8)\n",
    "\n",
    "    u9 = layers.Conv2DTranspose(64, (2, 2), strides=(2, 2), padding='same')(c8)\n",
    "    u9 = layers.concatenate([u9, c1])\n",
    "    c9 = layers.Conv2D(64, (3, 3), activation='relu', padding='same')(u9)\n",
    "    c9 = layers.Conv2D(64, (3, 3), activation='relu', padding='same')(c9)\n",
    "\n",
    "    outputs = layers.Conv2D(1, (1, 1), activation='sigmoid')(c9)\n",
    "\n",
    "    model = models.Model(inputs=[inputs], outputs=[outputs])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fdc04b6-ed27-4b2b-99f9-4fc920e74189",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the model\n",
    "input_shape = (128, 128, 1)\n",
    "unet = unet_model(input_shape)\n",
    "unet.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "367bf516-378b-4050-9e45-98cf8ac307aa",
   "metadata": {},
   "source": [
    "### Binary Crossentropy Loss Function\n",
    "\n",
    "The binary crossentropy loss function, also known as log loss, is a loss function commonly used in binary classification problems. It measures the performance of a classification model whose output is a probability value between 0 and 1.\n",
    "\n",
    "### Formula\n",
    "\n",
    "For a single instance, the binary crossentropy loss can be defined as:\n",
    "\n",
    "$$\n",
    "\\[ \\text{Binary Crossentropy} = - \\left( y \\cdot \\log(p) + (1 - y) \\cdot \\log(1 - p) \\right) \\]\n",
    "$$\n",
    "\n",
    "Where:\n",
    "\n",
    "- \\( y \\) is the actual label (0 or 1).\n",
    "- \\( p \\) is the predicted probability of the instance being in class 1.\n",
    "\n",
    "For a dataset with multiple instances, the binary crossentropy loss is averaged over all instances:\n",
    "\n",
    "$$\n",
    "\\[ \\text{Binary Crossentropy} = - \\frac{1}{N} \\sum_{i=1}^{N} \\left( y_i \\cdot \\log(p_i) + (1 - y_i) \\cdot \\log(1 - p_i) \\right) \\]\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- \\( N \\) is the number of instances.\n",
    "- \\( y_i \\) is the actual label of the \\(i\\)-th instance.\n",
    "- \\( p_i \\) is the predicted probability of the \\(i\\)-th instance being in class 1.\n",
    "\n",
    "### Intuition\n",
    "\n",
    "1. **Correct Predictions:** If the model predicts a probability close to the actual class (e.g., 0.9 for class 1 or 1.1 for class 0), the loss will be low.\n",
    "2. **Incorrect Predictions:** If the model predicts a probability far from the actual class (e.g., 0.1 for class 1 or 0.9 for class 0), the loss will be high.\n",
    "\n",
    "### Why Use Binary Crossentropy?\n",
    "\n",
    "- **Probabilistic Interpretation:** Binary crossentropy is suitable for models that output probabilities. It effectively penalizes the model more as the predicted probability diverges from the actual label.\n",
    "- **Differentiability:** The function is differentiable, which makes it suitable for gradient-based optimization algorithms like stochastic gradient descent (SGD).\n",
    "- **Focus on Confident Predictions:** The logarithmic nature of the loss function ensures that the model gets penalized more for making confident but wrong predictions than for uncertain predictions.\n",
    "\n",
    "### Use in Neural Networks\n",
    "\n",
    "In neural networks, binary crossentropy is commonly used as a loss function when the output layer has a sigmoid activation function. The sigmoid activation function maps the output to a probability value between 0 and 1, making it compatible with binary crossentropy.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23e79c38-5b1e-4574-bb8d-cb6460a40b72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assume train_images, train_masks, test_images, test_masks are predefined datasets\n",
    "# Train the model\n",
    "history = unet.fit(train_images, train_masks, epochs=50, batch_size=32, validation_data=(test_images, test_masks))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47b3b3a5-3a02-485d-8dee-5a0653f44b9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model\n",
    "def evaluate_unet_model(model: models.Model, test_images: np.ndarray, test_masks: np.ndarray) -> Tuple[float, float]:\n",
    "    \"\"\"\n",
    "    Evaluates the U-Net model on the test dataset.\n",
    "\n",
    "    Args:\n",
    "    model (models.Model): The trained U-Net model.\n",
    "    test_images (np.ndarray): Test images.\n",
    "    test_masks (np.ndarray): Test masks.\n",
    "\n",
    "    Returns:\n",
    "    Tuple[float, float]: Test loss and test accuracy.\n",
    "    \"\"\"\n",
    "    test_loss, test_acc = model.evaluate(test_images, test_masks, verbose=2)\n",
    "    return test_loss, test_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ddd5463-4df4-4e53-bd98-ecbee54add19",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loss, test_acc = evaluate_unet_model(unet, test_images, test_masks)\n",
    "print(f'Test accuracy: {test_acc * 100:.2f}%')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abf0977f-3aba-4595-a7ac-2f1736c622b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize predictions\n",
    "def visualize_unet_predictions(model: models.Model, test_images: np.ndarray, test_masks: np.ndarray, num_images: int = 5) -> None:\n",
    "    \"\"\"\n",
    "    Visualizes predictions of the U-Net model on sample images from the test dataset.\n",
    "\n",
    "    Args:\n",
    "    model (models.Model): The trained U-Net model.\n",
    "    test_images (np.ndarray): Test images.\n",
    "    test_masks (np.ndarray): Actual masks for the test images.\n",
    "    num_images (int): Number of images to visualize.\n",
    "    \"\"\"\n",
    "    predictions = model.predict(test_images[:num_images])\n",
    "\n",
    "    for i in range(num_images):\n",
    "        plt.figure(figsize=(12, 6))\n",
    "        plt.subplot(1, 3, 1)\n",
    "        plt.imshow(test_images[i].squeeze(), cmap='gray')\n",
    "        plt.title('Input Image')\n",
    "        plt.subplot(1, 3, 2)\n",
    "        plt.imshow(test_masks[i].squeeze(), cmap='gray')\n",
    "        plt.title('True Mask')\n",
    "        plt.subplot(1, 3, 3)\n",
    "        plt.imshow(predictions[i].squeeze(), cmap='gray')\n",
    "        plt.title('Predicted Mask')\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1399b145-8fdb-4384-bc8f-0c3c4ac432d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the predictions on some test images\n",
    "visualize_unet_predictions(unet, test_images, test_masks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "c58fe92b-41af-440b-b5f8-81ac52b358ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow version: 2.16.1\n",
      "Num GPUs Available:  0\n",
      "Available GPUs:  []\n",
      "Epoch 1/20\n",
      "\u001b[1m 82/844\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1:45:01\u001b[0m 8s/step - loss: 0.0175"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[29], line 81\u001b[0m\n\u001b[0;32m     78\u001b[0m model\u001b[38;5;241m.\u001b[39mcompile(optimizer\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124madam\u001b[39m\u001b[38;5;124m'\u001b[39m, loss\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmean_squared_error\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     80\u001b[0m \u001b[38;5;66;03m# Train the model\u001b[39;00m\n\u001b[1;32m---> 81\u001b[0m history \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m20\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     83\u001b[0m \u001b[38;5;66;03m# Plot training loss values\u001b[39;00m\n\u001b[0;32m     84\u001b[0m plt\u001b[38;5;241m.\u001b[39mplot(history\u001b[38;5;241m.\u001b[39mhistory[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mloss\u001b[39m\u001b[38;5;124m'\u001b[39m])\n",
      "File \u001b[1;32mc:\\users\\bbrel\\esa_webinar\\.venv\\lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:117\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    115\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    116\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 117\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    118\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32mc:\\users\\bbrel\\esa_webinar\\.venv\\lib\\site-packages\\keras\\src\\backend\\tensorflow\\trainer.py:314\u001b[0m, in \u001b[0;36mTensorFlowTrainer.fit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq)\u001b[0m\n\u001b[0;32m    312\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m step, iterator \u001b[38;5;129;01min\u001b[39;00m epoch_iterator\u001b[38;5;241m.\u001b[39menumerate_epoch():\n\u001b[0;32m    313\u001b[0m     callbacks\u001b[38;5;241m.\u001b[39mon_train_batch_begin(step)\n\u001b[1;32m--> 314\u001b[0m     logs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    315\u001b[0m     logs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pythonify_logs(logs)\n\u001b[0;32m    316\u001b[0m     callbacks\u001b[38;5;241m.\u001b[39mon_train_batch_end(step, logs)\n",
      "File \u001b[1;32mc:\\users\\bbrel\\esa_webinar\\.venv\\lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    148\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    149\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 150\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32mc:\\users\\bbrel\\esa_webinar\\.venv\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\polymorphic_function.py:833\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    830\u001b[0m compiler \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxla\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnonXla\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    832\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m OptionalXlaContext(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile):\n\u001b[1;32m--> 833\u001b[0m   result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[0;32m    835\u001b[0m new_tracing_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexperimental_get_tracing_count()\n\u001b[0;32m    836\u001b[0m without_tracing \u001b[38;5;241m=\u001b[39m (tracing_count \u001b[38;5;241m==\u001b[39m new_tracing_count)\n",
      "File \u001b[1;32mc:\\users\\bbrel\\esa_webinar\\.venv\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\polymorphic_function.py:878\u001b[0m, in \u001b[0;36mFunction._call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    875\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n\u001b[0;32m    876\u001b[0m \u001b[38;5;66;03m# In this case we have not created variables on the first call. So we can\u001b[39;00m\n\u001b[0;32m    877\u001b[0m \u001b[38;5;66;03m# run the first trace but we should fail if variables are created.\u001b[39;00m\n\u001b[1;32m--> 878\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[43mtracing_compilation\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    879\u001b[0m \u001b[43m    \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_variable_creation_config\u001b[49m\n\u001b[0;32m    880\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    881\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_created_variables:\n\u001b[0;32m    882\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCreating variables on a non-first call to a function\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    883\u001b[0m                    \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m decorated with tf.function.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\users\\bbrel\\esa_webinar\\.venv\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\tracing_compilation.py:139\u001b[0m, in \u001b[0;36mcall_function\u001b[1;34m(args, kwargs, tracing_options)\u001b[0m\n\u001b[0;32m    137\u001b[0m bound_args \u001b[38;5;241m=\u001b[39m function\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39mbind(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    138\u001b[0m flat_inputs \u001b[38;5;241m=\u001b[39m function\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39munpack_inputs(bound_args)\n\u001b[1;32m--> 139\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_flat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# pylint: disable=protected-access\u001b[39;49;00m\n\u001b[0;32m    140\u001b[0m \u001b[43m    \u001b[49m\u001b[43mflat_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcaptured_inputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfunction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcaptured_inputs\u001b[49m\n\u001b[0;32m    141\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\users\\bbrel\\esa_webinar\\.venv\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\concrete_function.py:1322\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[1;34m(self, tensor_inputs, captured_inputs)\u001b[0m\n\u001b[0;32m   1318\u001b[0m possible_gradient_type \u001b[38;5;241m=\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[0;32m   1319\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (possible_gradient_type \u001b[38;5;241m==\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[0;32m   1320\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m executing_eagerly):\n\u001b[0;32m   1321\u001b[0m   \u001b[38;5;66;03m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[1;32m-> 1322\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_inference_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_preflattened\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1323\u001b[0m forward_backward \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[0;32m   1324\u001b[0m     args,\n\u001b[0;32m   1325\u001b[0m     possible_gradient_type,\n\u001b[0;32m   1326\u001b[0m     executing_eagerly)\n\u001b[0;32m   1327\u001b[0m forward_function, args_with_tangents \u001b[38;5;241m=\u001b[39m forward_backward\u001b[38;5;241m.\u001b[39mforward()\n",
      "File \u001b[1;32mc:\\users\\bbrel\\esa_webinar\\.venv\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\atomic_function.py:216\u001b[0m, in \u001b[0;36mAtomicFunction.call_preflattened\u001b[1;34m(self, args)\u001b[0m\n\u001b[0;32m    214\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcall_preflattened\u001b[39m(\u001b[38;5;28mself\u001b[39m, args: Sequence[core\u001b[38;5;241m.\u001b[39mTensor]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[0;32m    215\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"Calls with flattened tensor inputs and returns the structured output.\"\"\"\u001b[39;00m\n\u001b[1;32m--> 216\u001b[0m   flat_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_flat\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    217\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39mpack_output(flat_outputs)\n",
      "File \u001b[1;32mc:\\users\\bbrel\\esa_webinar\\.venv\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\atomic_function.py:251\u001b[0m, in \u001b[0;36mAtomicFunction.call_flat\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m    249\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m record\u001b[38;5;241m.\u001b[39mstop_recording():\n\u001b[0;32m    250\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_bound_context\u001b[38;5;241m.\u001b[39mexecuting_eagerly():\n\u001b[1;32m--> 251\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_bound_context\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    252\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    253\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    254\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunction_type\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mflat_outputs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    255\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    256\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    257\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m make_call_op_in_graph(\n\u001b[0;32m    258\u001b[0m         \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    259\u001b[0m         \u001b[38;5;28mlist\u001b[39m(args),\n\u001b[0;32m    260\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_bound_context\u001b[38;5;241m.\u001b[39mfunction_call_options\u001b[38;5;241m.\u001b[39mas_attrs(),\n\u001b[0;32m    261\u001b[0m     )\n",
      "File \u001b[1;32mc:\\users\\bbrel\\esa_webinar\\.venv\\lib\\site-packages\\tensorflow\\python\\eager\\context.py:1500\u001b[0m, in \u001b[0;36mContext.call_function\u001b[1;34m(self, name, tensor_inputs, num_outputs)\u001b[0m\n\u001b[0;32m   1498\u001b[0m cancellation_context \u001b[38;5;241m=\u001b[39m cancellation\u001b[38;5;241m.\u001b[39mcontext()\n\u001b[0;32m   1499\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cancellation_context \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 1500\u001b[0m   outputs \u001b[38;5;241m=\u001b[39m \u001b[43mexecute\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1501\u001b[0m \u001b[43m      \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mutf-8\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1502\u001b[0m \u001b[43m      \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1503\u001b[0m \u001b[43m      \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtensor_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1504\u001b[0m \u001b[43m      \u001b[49m\u001b[43mattrs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1505\u001b[0m \u001b[43m      \u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1506\u001b[0m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1507\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1508\u001b[0m   outputs \u001b[38;5;241m=\u001b[39m execute\u001b[38;5;241m.\u001b[39mexecute_with_cancellation(\n\u001b[0;32m   1509\u001b[0m       name\u001b[38;5;241m.\u001b[39mdecode(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m   1510\u001b[0m       num_outputs\u001b[38;5;241m=\u001b[39mnum_outputs,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1514\u001b[0m       cancellation_manager\u001b[38;5;241m=\u001b[39mcancellation_context,\n\u001b[0;32m   1515\u001b[0m   )\n",
      "File \u001b[1;32mc:\\users\\bbrel\\esa_webinar\\.venv\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py:53\u001b[0m, in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     51\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     52\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[1;32m---> 53\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m \u001b[43mpywrap_tfe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTFE_Py_Execute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_handle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     54\u001b[0m \u001b[43m                                      \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     55\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     56\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow_datasets as tfds\n",
    "\n",
    "# Load and preprocess the EuroSAT dataset\n",
    "def preprocess_image(image, label):\n",
    "    image = tf.image.resize(image, [128, 128])  # Resize to 128x128\n",
    "    image = image / 255.0  # Normalize to [0, 1]\n",
    "    return image, image  # Use image as both input and target for autoencoding/segmentation tasks\n",
    "\n",
    "# Load EuroSAT dataset\n",
    "dataset, info = tfds.load('eurosat', split='train', with_info=True, as_supervised=True)\n",
    "dataset = dataset.map(preprocess_image).batch(32)\n",
    "\n",
    "# Define the U-Net model\n",
    "def unet_model(input_size=(128, 128, 3)):\n",
    "    inputs = tf.keras.Input(shape=input_size)\n",
    "\n",
    "    # Encoder\n",
    "    c1 = layers.Conv2D(64, (3, 3), activation='relu', padding='same')(inputs)\n",
    "    c1 = layers.Conv2D(64, (3, 3), activation='relu', padding='same')(c1)\n",
    "    p1 = layers.MaxPooling2D((2, 2))(c1)\n",
    "\n",
    "    c2 = layers.Conv2D(128, (3, 3), activation='relu', padding='same')(p1)\n",
    "    c2 = layers.Conv2D(128, (3, 3), activation='relu', padding='same')(c2)\n",
    "    p2 = layers.MaxPooling2D((2, 2))(c2)\n",
    "\n",
    "    c3 = layers.Conv2D(256, (3, 3), activation='relu', padding='same')(p2)\n",
    "    c3 = layers.Conv2D(256, (3, 3), activation='relu', padding='same')(c3)\n",
    "    p3 = layers.MaxPooling2D((2, 2))(c3)\n",
    "\n",
    "    c4 = layers.Conv2D(512, (3, 3), activation='relu', padding='same')(p3)\n",
    "    c4 = layers.Conv2D(512, (3, 3), activation='relu', padding='same')(c4)\n",
    "    p4 = layers.MaxPooling2D((2, 2))(c4)\n",
    "\n",
    "    # Bottleneck\n",
    "    c5 = layers.Conv2D(1024, (3, 3), activation='relu', padding='same')(p4)\n",
    "    c5 = layers.Conv2D(1024, (3, 3), activation='relu', padding='same')(c5)\n",
    "\n",
    "    # Decoder\n",
    "    u6 = layers.Conv2DTranspose(512, (2, 2), strides=(2, 2), padding='same')(c5)\n",
    "    u6 = layers.concatenate([u6, c4])\n",
    "    c6 = layers.Conv2D(512, (3, 3), activation='relu', padding='same')(u6)\n",
    "    c6 = layers.Conv2D(512, (3, 3), activation='relu', padding='same')(c6)\n",
    "\n",
    "    u7 = layers.Conv2DTranspose(256, (2, 2), strides=(2, 2), padding='same')(c6)\n",
    "    u7 = layers.concatenate([u7, c3])\n",
    "    c7 = layers.Conv2D(256, (3, 3), activation='relu', padding='same')(u7)\n",
    "    c7 = layers.Conv2D(256, (3, 3), activation='relu', padding='same')(c7)\n",
    "\n",
    "    u8 = layers.Conv2DTranspose(128, (2, 2), strides=(2, 2), padding='same')(c7)\n",
    "    u8 = layers.concatenate([u8, c2])\n",
    "    c8 = layers.Conv2D(128, (3, 3), activation='relu', padding='same')(u8)\n",
    "    c8 = layers.Conv2D(128, (3, 3), activation='relu', padding='same')(c8)\n",
    "\n",
    "    u9 = layers.Conv2DTranspose(64, (2, 2), strides=(2, 2), padding='same')(c8)\n",
    "    u9 = layers.concatenate([u9, c1])\n",
    "    c9 = layers.Conv2D(64, (3, 3), activation='relu', padding='same')(u9)\n",
    "    c9 = layers.Conv2D(64, (3, 3), activation='relu', padding='same')(c9)\n",
    "\n",
    "    outputs = layers.Conv2D(3, (1, 1), activation='sigmoid')(c9)\n",
    "\n",
    "    model = models.Model(inputs=[inputs], outputs=[outputs])\n",
    "    return model\n",
    "\n",
    "# Build and compile the U-Net model\n",
    "model = unet_model()\n",
    "model.compile(optimizer='adam', loss='mean_squared_error')\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(dataset, epochs=20)\n",
    "\n",
    "# Plot training loss values\n",
    "plt.plot(history.history['loss'])\n",
    "plt.title('Model loss')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train'], loc='upper left')\n",
    "plt.show()\n",
    "\n",
    "# Predict and display some example results\n",
    "def display_predictions(dataset, model, num_images=5):\n",
    "    for images, labels in dataset.take(1):\n",
    "        predictions = model.predict(images)\n",
    "        plt.figure(figsize=(10, 10))\n",
    "        for i in range(num_images):\n",
    "            plt.subplot(num_images, 3, i * 3 + 1)\n",
    "            plt.imshow(images[i])\n",
    "            plt.title('Input')\n",
    "            plt.axis('off')\n",
    "\n",
    "            plt.subplot(num_images, 3, i * 3 + 2)\n",
    "            plt.imshow(predictions[i])\n",
    "            plt.title('Prediction')\n",
    "            plt.axis('off')\n",
    "\n",
    "            plt.subplot(num_images, 3, i * 3 + 3)\n",
    "            plt.imshow(labels[i])\n",
    "            plt.title('Ground Truth')\n",
    "            plt.axis('off')\n",
    "        plt.show()\n",
    "\n",
    "# Display predictions\n",
    "display_predictions(dataset, model)\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e62f0c3-d8e2-43ef-b7c3-bee0b415a0a4",
   "metadata": {},
   "source": [
    "## Attention Model\n",
    "\n",
    "This section demonstrates how to create, compile, train, and visualize an Attention model. The attention mechanism allows the model to focus on different parts of the input sequence, enhancing its ability to handle long-range dependencies and improve performance on tasks like image classification and natural language processing.\n",
    "\n",
    "### Steps:\n",
    "1. **Create the Attention Layer.**\n",
    "2. **Build the Attention model.**\n",
    "3. **Compile the model with an appropriate optimizer and loss function.**\n",
    "4. **Train the model on a dataset.**\n",
    "5. **Evaluate the model's performance.**\n",
    "6. **Visualize the attention weights.**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49638c8e-c91e-4312-80cd-e11593ab6a2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the Attention Layer class\n",
    "class AttentionLayer(layers.Layer):\n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape: Tuple[int, ...]) -> None:\n",
    "        self.W = self.add_weight(name='attention_weight', shape=(input_shape[-1], 1), initializer='random_normal', trainable=True)\n",
    "        self.b = self.add_weight(name='attention_bias', shape=(input_shape[1], 1), initializer='zeros', trainable=True)\n",
    "        super().build(input_shape)\n",
    "\n",
    "    def call(self, inputs: tf.Tensor, **kwargs) -> tf.Tensor:\n",
    "        e = tf.nn.tanh(tf.tensordot(inputs, self.W, axes=1) + self.b)\n",
    "        alpha = tf.nn.softmax(e, axis=1)\n",
    "        context = tf.reduce_sum(alpha * inputs, axis=1)\n",
    "        return context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df373295-8534-42d6-8665-1bca6b16e589",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to build the Attention model\n",
    "def build_attention_model(input_shape: Tuple[int, int, int]) -> models.Model:\n",
    "    \"\"\"\n",
    "    Builds a Convolutional Neural Network (CNN) model with an attention layer.\n",
    "\n",
    "    Args:\n",
    "    input_shape (Tuple[int, int, int]): Shape of the input images.\n",
    "\n",
    "    Returns:\n",
    "    models.Model: Compiled CNN model with attention.\n",
    "    \"\"\"\n",
    "    inputs = layers.Input(shape=input_shape)\n",
    "    x = layers.Conv2D(32, (3, 3), activation='relu', padding='same')(inputs)\n",
    "    x = layers.MaxPooling2D((2, 2))(x)\n",
    "    x = layers.Conv2D(64, (3, 3), activation='relu', padding='same')(x)\n",
    "    x = layers.MaxPooling2D((2, 2))(x)\n",
    "    x = layers.Conv2D(128, (3, 3), activation='relu', padding='same')(x)\n",
    "    x = layers.MaxPooling2D((2, 2))(x)\n",
    "    x = layers.Flatten()(x)\n",
    "    x = layers.Reshape((-1, 128))(x)  # Reshape for the attention layer\n",
    "    attention = AttentionLayer()(x)\n",
    "    outputs = layers.Dense(10, activation='softmax')(attention)\n",
    "    model = models.Model(inputs, outputs)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13b28034-dbe4-4a5b-bbc4-06c40f3ddfec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build and compile the model\n",
    "input_shape = (64, 64, 3)\n",
    "model = build_attention_model(input_shape)\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5894e1c7-ac17-4432-84c8-fc011d9e60a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assume `dataset` is a preprocessed dataset\n",
    "# Train the model\n",
    "history = model.fit(dataset, epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91d10f10-3e70-4962-a5d8-15be53207dc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training accuracy values\n",
    "plt.plot(history.history['accuracy'])\n",
    "plt.title('Model accuracy')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43001e22-27f6-44ce-bad3-661f98d7a6d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training loss values\n",
    "plt.plot(history.history['loss'])\n",
    "plt.title('Model loss')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59e315c5-7982-47b4-88f0-cd12eba023d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model\n",
    "def evaluate_model(dataset: tf.data.Dataset, model: models.Model) -> None:\n",
    "    \"\"\"\n",
    "    Evaluates the model on the provided dataset.\n",
    "\n",
    "    Args:\n",
    "    dataset (tf.data.Dataset): Dataset for evaluation.\n",
    "    model (models.Model): Trained model to be evaluated.\n",
    "    \"\"\"\n",
    "    accuracy = model.evaluate(dataset)\n",
    "    print(f'Accuracy: {accuracy[1] * 100:.2f}%')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c53bd5dc-4177-4d9d-9ae3-e77d7ba69952",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_model(dataset, model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8398ecf-85f1-478d-9c93-420161e56e05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize Attention Weights\n",
    "def visualize_attention_weights(image: tf.Tensor, model: models.Model) -> None:\n",
    "    \"\"\"\n",
    "    Visualizes the attention weights on a sample image.\n",
    "\n",
    "    Args:\n",
    "    image (tf.Tensor): Input image.\n",
    "    model (models.Model): Trained model with an attention layer.\n",
    "    \"\"\"\n",
    "    attention_layer_model = models.Model(inputs=model.input, outputs=model.get_layer('attention_layer').output)\n",
    "    attention_weights = attention_layer_model.predict(tf.expand_dims(image, axis=0))[0]\n",
    "\n",
    "    plt.imshow(image)\n",
    "    plt.imshow(attention_weights, cmap='jet', alpha=0.5)\n",
    "    plt.title('Attention Map')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9242b3fd-0a5c-49f6-98e3-1df4b9757f40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize attention on some test images\n",
    "for images, _ in dataset.take(1):\n",
    "    for i in range(5):\n",
    "        visualize_attention_weights(images[i], model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "fb1cc90c-ffbd-4787-831a-41b64e2d0603",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow version: 2.16.1\n",
      "Num GPUs Available:  0\n",
      "Available GPUs:  []\n",
      "Epoch 1/10\n",
      "\u001b[1m516/844\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 57ms/step - accuracy: 0.3312 - loss: 1.7649"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_29520\\1744852311.py\u001b[0m in \u001b[0;36m?\u001b[1;34m()\u001b[0m\n\u001b[1;32m---> 69\u001b[1;33m \u001b[1;31m# Import necessary libraries\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     70\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     71\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeras\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mlayers\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbackend\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mK\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     72\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\bbrel\\esa_webinar\\.venv\\lib\\site-packages\\keras\\src\\utils\\traceback_utils.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    120\u001b[0m             \u001b[1;31m# To get the full stack trace, call:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    121\u001b[0m             \u001b[1;31m# `keras.config.disable_traceback_filtering()`\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    122\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    123\u001b[0m         \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 124\u001b[1;33m             \u001b[1;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mc:\\users\\bbrel\\esa_webinar\\.venv\\lib\\site-packages\\keras\\src\\backend\\tensorflow\\trainer.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq)\u001b[0m\n\u001b[0;32m    314\u001b[0m                     \u001b[0mlogs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    315\u001b[0m                     \u001b[0mlogs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_pythonify_logs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlogs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    316\u001b[0m                     \u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_train_batch_end\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    317\u001b[0m                     \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstop_training\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 318\u001b[1;33m                         \u001b[1;32mbreak\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    319\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    320\u001b[0m             \u001b[1;31m# Override with model metrics instead of last step logs if needed.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    321\u001b[0m             \u001b[0mepoch_logs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_metrics_result_or_logs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlogs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\bbrel\\esa_webinar\\.venv\\lib\\site-packages\\keras\\src\\callbacks\\callback_list.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(self, batch, logs)\u001b[0m\n\u001b[0;32m    103\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mon_train_batch_end\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    104\u001b[0m         \u001b[0mlogs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlogs\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    105\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mcallback\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 106\u001b[1;33m             \u001b[0mcallback\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_train_batch_end\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlogs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mc:\\users\\bbrel\\esa_webinar\\.venv\\lib\\site-packages\\keras\\src\\callbacks\\progbar_logger.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(self, batch, logs)\u001b[0m\n\u001b[0;32m     57\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mon_train_batch_end\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 58\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_update_progbar\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mc:\\users\\bbrel\\esa_webinar\\.venv\\lib\\site-packages\\keras\\src\\callbacks\\progbar_logger.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(self, batch, logs)\u001b[0m\n\u001b[0;32m     91\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_maybe_init_progbar\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     92\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mseen\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbatch\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m1\u001b[0m  \u001b[1;31m# One-indexed.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     93\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     94\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mverbose\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 95\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprogbar\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mseen\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlogs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfinalize\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mc:\\users\\bbrel\\esa_webinar\\.venv\\lib\\site-packages\\keras\\src\\utils\\progbar.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(self, current, values, finalize)\u001b[0m\n\u001b[0;32m    159\u001b[0m             \u001b[1;32mfor\u001b[0m \u001b[0mk\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_values_order\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    160\u001b[0m                 \u001b[0minfo\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;34mf\" - {k}:\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    161\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_values\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    162\u001b[0m                     avg = backend.convert_to_numpy(\n\u001b[1;32m--> 163\u001b[1;33m                         backend.numpy.mean(\n\u001b[0m\u001b[0;32m    164\u001b[0m                             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_values\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m/\u001b[0m \u001b[0mmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_values\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    165\u001b[0m                         )\n\u001b[0;32m    166\u001b[0m                     )\n",
      "\u001b[1;32mc:\\users\\bbrel\\esa_webinar\\.venv\\lib\\site-packages\\keras\\src\\backend\\tensorflow\\numpy.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(x, axis, keepdims)\u001b[0m\n\u001b[0;32m    567\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;34m\"int\"\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mori_dtype\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mori_dtype\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"bool\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    568\u001b[0m         \u001b[0mresult_dtype\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcompute_dtype\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    569\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    570\u001b[0m         \u001b[0mresult_dtype\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mori_dtype\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 571\u001b[1;33m     output = tf.reduce_mean(\n\u001b[0m\u001b[0;32m    572\u001b[0m         \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcast\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcompute_dtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkeepdims\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mkeepdims\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    573\u001b[0m     )\n\u001b[0;32m    574\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcast\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult_dtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\bbrel\\esa_webinar\\.venv\\lib\\site-packages\\tensorflow\\python\\ops\\weak_tensor_ops.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     86\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     87\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_auto_dtype_conversion_enabled\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 88\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     89\u001b[0m     \u001b[0mbound_arguments\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msignature\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbind\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     90\u001b[0m     \u001b[0mbound_arguments\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply_defaults\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     91\u001b[0m     \u001b[0mbound_kwargs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbound_arguments\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marguments\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\bbrel\\esa_webinar\\.venv\\lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    151\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    153\u001b[0m       \u001b[1;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    154\u001b[0m     \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 155\u001b[1;33m       \u001b[1;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mc:\\users\\bbrel\\esa_webinar\\.venv\\lib\\site-packages\\tensorflow\\python\\util\\dispatch.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m   1264\u001b[0m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdispatch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mop_dispatch_handler\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1265\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mresult\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mOpDispatcher\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mNOT_SUPPORTED\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1266\u001b[0m           \u001b[1;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1267\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1268\u001b[1;33m           \u001b[1;32mraise\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mc:\\users\\bbrel\\esa_webinar\\.venv\\lib\\site-packages\\tensorflow\\python\\ops\\math_ops.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(input_tensor, axis, keepdims, name)\u001b[0m\n\u001b[0;32m   2542\u001b[0m   \u001b[0mkeepdims\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mkeepdims\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0mbool\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkeepdims\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2543\u001b[0m   return _may_reduce_to_scalar(\n\u001b[0;32m   2544\u001b[0m       \u001b[0mkeepdims\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2545\u001b[0m       gen_math_ops.mean(\n\u001b[1;32m-> 2546\u001b[1;33m           \u001b[0minput_tensor\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_ReductionDims\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_tensor\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkeepdims\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2547\u001b[0m           name=name))\n",
      "\u001b[1;32mc:\\users\\bbrel\\esa_webinar\\.venv\\lib\\site-packages\\tensorflow\\python\\ops\\math_ops.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(x, axis)\u001b[0m\n\u001b[0;32m   2046\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mx_rank\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2047\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0mconstant_op\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconstant\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_rank\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mint32\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2048\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2049\u001b[0m       \u001b[1;31m# Otherwise, we rely on Range and Rank to do the right thing at run-time.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2050\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0marray_ops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrank\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mc:\\users\\bbrel\\esa_webinar\\.venv\\lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    151\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    153\u001b[0m       \u001b[1;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    154\u001b[0m     \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 155\u001b[1;33m       \u001b[1;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mc:\\users\\bbrel\\esa_webinar\\.venv\\lib\\site-packages\\tensorflow\\python\\util\\dispatch.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m   1264\u001b[0m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdispatch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mop_dispatch_handler\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1265\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mresult\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mOpDispatcher\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mNOT_SUPPORTED\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1266\u001b[0m           \u001b[1;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1267\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1268\u001b[1;33m           \u001b[1;32mraise\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mc:\\users\\bbrel\\esa_webinar\\.venv\\lib\\site-packages\\tensorflow\\python\\ops\\math_ops.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(start, limit, delta, dtype, name)\u001b[0m\n\u001b[0;32m   2017\u001b[0m     \u001b[0mstart\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcast\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstart\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minferred_dtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2018\u001b[0m     \u001b[0mlimit\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcast\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlimit\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minferred_dtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2019\u001b[0m     \u001b[0mdelta\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcast\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdelta\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minferred_dtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2020\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2021\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mgen_math_ops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_range\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstart\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlimit\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdelta\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mc:\\users\\bbrel\\esa_webinar\\.venv\\lib\\site-packages\\tensorflow\\python\\ops\\gen_math_ops.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(start, limit, delta, name)\u001b[0m\n\u001b[0;32m   8021\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0m_result\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   8022\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   8023\u001b[0m       \u001b[0m_ops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mraise_from_not_ok_status\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   8024\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_FallbackException\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 8025\u001b[1;33m       \u001b[1;32mpass\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   8026\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   8027\u001b[0m       return _range_eager_fallback(\n\u001b[0;32m   8028\u001b[0m           start, limit, delta, name=name, ctx=_ctx)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "'''\n",
    "\n",
    "# Define an Attention Layer\n",
    "class AttentionLayer(layers.Layer):\n",
    "    def __init__(self, **kwargs):\n",
    "        super(AttentionLayer, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.W = self.add_weight(shape=(input_shape[-1], input_shape[-1]),\n",
    "                                 initializer='random_normal', trainable=True)\n",
    "        self.b = self.add_weight(shape=(input_shape[-1],),\n",
    "                                 initializer='random_normal', trainable=True)\n",
    "        self.u = self.add_weight(shape=(input_shape[-1],),\n",
    "                                 initializer='random_normal', trainable=True)\n",
    "        super(AttentionLayer, self).build(input_shape)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        u_t = tf.tanh(tf.tensordot(inputs, self.W, axes=[-1, 0]) + self.b)\n",
    "        a_t = tf.tensordot(u_t, self.u, axes=[-1, 0])\n",
    "        a_t = tf.nn.softmax(a_t)\n",
    "        output = inputs * tf.expand_dims(a_t, -1)\n",
    "        return tf.reduce_sum(output, axis=1)\n",
    "\n",
    "# Define the model with attention mechanism\n",
    "def build_attention_model(input_shape):\n",
    "    inputs = layers.Input(shape=input_shape)\n",
    "    x = layers.Conv2D(32, (3, 3), activation='relu', padding='same')(inputs)\n",
    "    x = layers.MaxPooling2D((2, 2))(x)\n",
    "    x = layers.Conv2D(64, (3, 3), activation='relu', padding='same')(x)\n",
    "    x = layers.MaxPooling2D((2, 2))(x)\n",
    "    x = layers.Conv2D(128, (3, 3), activation='relu', padding='same')(x)\n",
    "    x = layers.MaxPooling2D((2, 2))(x)\n",
    "    x = layers.Flatten()(x)\n",
    "    x = layers.Reshape((-1, 128))(x)  # Reshape for the attention layer\n",
    "    attention = AttentionLayer()(x)\n",
    "    outputs = layers.Dense(10, activation='softmax')(attention)\n",
    "    model = models.Model(inputs, outputs)\n",
    "    return model\n",
    "\n",
    "# Build and compile the model\n",
    "input_shape = (64, 64, 3)\n",
    "model = build_attention_model(input_shape)\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(dataset, epochs=10)\n",
    "\n",
    "# Plot training accuracy values\n",
    "plt.plot(history.history['accuracy'])\n",
    "plt.title('Model accuracy')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train'], loc='upper left')\n",
    "plt.show()\n",
    "\n",
    "# Plot training loss values\n",
    "plt.plot(history.history['loss'])\n",
    "plt.title('Model loss')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train'], loc='upper left')\n",
    "plt.show()\n",
    "\n",
    "# Evaluate the model\n",
    "def evaluate_model(dataset, model):\n",
    "    accuracy = model.evaluate(dataset)\n",
    "    print(f'Accuracy: {accuracy[1] * 100:.2f}%')\n",
    "\n",
    "evaluate_model(dataset, model)\n",
    "\n",
    "# Visualize Attention Weights\n",
    "def visualize_attention_weights(image, model):\n",
    "    attention_layer_model = models.Model(inputs=model.input, outputs=model.get_layer('attention_layer').output)\n",
    "    attention_weights = attention_layer_model.predict(tf.expand_dims(image, axis=0))[0]\n",
    "    \n",
    "    plt.imshow(image)\n",
    "    plt.imshow(attention_weights, cmap='jet', alpha=0.5)\n",
    "    plt.title('Attention Map')\n",
    "    plt.show()\n",
    "\n",
    "# Visualize attention on some test images\n",
    "for images, _ in dataset.take(1):\n",
    "    for i in range(5):\n",
    "        visualize_attention_weights(images[i], model)\n",
    "'''"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
